
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://yourwebsite.com/optimization/unconstrained/line_vs_trust/">
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Two Strategies in Unconstrained Optimization: Line Search vs. Trust Region - PhD Notes: Optimization & Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Lato";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="orange" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#two-strategies-in-unconstrained-optimization-line-search-vs-trust-region" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="PhD Notes: Optimization &amp; Machine Learning" class="md-header__button md-logo" aria-label="PhD Notes: Optimization & Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            PhD Notes: Optimization & Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Two Strategies in Unconstrained Optimization: Line Search vs. Trust Region
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="orange" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="blue-grey" data-md-color-accent="amber"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../index.md" class="md-tabs__link">
          
  
  
  Optimization

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="PhD Notes: Optimization &amp; Machine Learning" class="md-nav__button md-logo" aria-label="PhD Notes: Optimization & Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    PhD Notes: Optimization & Machine Learning
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="two-strategies-in-unconstrained-optimization-line-search-vs-trust-region">Two Strategies in Unconstrained Optimization: Line Search vs. Trust Region</h1>
<p>In iterative optimization, after determining a search direction $ p_k $ from the current iterate $ x_k $, one must decide how far to move along that direction. Two broad strategies to determine the step are:</p>
<ol>
<li>
<p><strong>Line Search Methods:</strong><br />
   Find a step size <span class="arithmatex">\(\alpha_k\)</span> such that the new iterate is
   <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>x_{k+1} = x_k + \alpha_k p_k,
</code></pre></div></td></tr></table></div>
   and <span class="arithmatex">\(\alpha_k\)</span> is chosen to guarantee sufficient decrease (or satisfy other conditions).</p>
</li>
<li>
<p><strong>Trust Region Methods:</strong><br />
   Instead of moving along a fixed direction with a step size, these methods first build a local model $ m_k(p) $ of the objective function around $ x_k $ and then solve a subproblem that restricts the step $ p $ to lie within a neighborhood (trust region) of $ x_k $. The update is then
   <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>x_{k+1} = x_k + p_k, \quad \text{with } p_k = \arg\min_{\|p\|\le \Delta_k} m_k(p),
</code></pre></div></td></tr></table></div>
   where <span class="arithmatex">\(\Delta_k\)</span> is the trust-region radius.</p>
</li>
</ol>
<p>Below we explore each strategy in more detail.</p>
<hr />
<h2 id="1-line-search-methods">1. Line Search Methods</h2>
<h3 id="11-concept-and-motivation">1.1. Concept and Motivation</h3>
<p>In a line search method, given a descent direction $ p_k $, we define the one-dimensional function
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\phi(\alpha) = f(x_k + \alpha p_k),
</code></pre></div></td></tr></table></div>
and seek a step length <span class="arithmatex">\(\alpha_k\)</span> that (approximately) minimizes <span class="arithmatex">\(\phi\)</span>. However, rather than computing the exact minimizer, <strong>inexact line search</strong> methods impose conditions to guarantee sufficient progress. A common requirement is the <strong>Armijo condition</strong>:
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>f(x_k + \alpha_k p_k) \le f(x_k) + c \alpha_k \nabla f(x_k)^T p_k,
</code></pre></div></td></tr></table></div>
with a small constant <span class="arithmatex">\(c \in (0,1)\)</span> (often <span class="arithmatex">\(c \approx 10^{-4}\)</span>).</p>
<p>To further prevent <span class="arithmatex">\(\alpha_k\)</span> from being too small, <strong>Wolfe conditions</strong> (or strong Wolfe conditions) add a curvature condition:
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\big| \nabla f(x_k + \alpha_k p_k)^T p_k \big| \le c_2 \big| \nabla f(x_k)^T p_k \big|,
</code></pre></div></td></tr></table></div>
with <span class="arithmatex">\(c_2\)</span> typically in <span class="arithmatex">\((c, 1)\)</span>.</p>
<h3 id="12-algorithm-backtracking-line-search">1.2. Algorithm: Backtracking Line Search</h3>
<p>A common practical approach is the backtracking line search:
1. <strong>Initialize:</strong> Set <span class="arithmatex">\(\alpha = \alpha_{\text{init}}\)</span> (often 1) and choose $ \tau \in (0,1) $ (e.g., 0.5).
2. <strong>Iterate:</strong> While the Armijo condition is not met, update <span class="arithmatex">\(\alpha \leftarrow \tau \alpha\)</span>.
3. <strong>Return:</strong> Use the final <span class="arithmatex">\(\alpha\)</span> as <span class="arithmatex">\(\alpha_k\)</span>.</p>
<h3 id="13-code-examples">1.3. Code Examples</h3>
<p>=== julia
<div class="language-julia highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c"># Define the objective function and its gradient.</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span>

<span class="c"># Backtracking line search function.</span>
<span class="k">function</span><span class="w"> </span><span class="n">backtracking_line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">grad_f</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">;</span><span class="w"> </span><span class="n">α_init</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="n">τ</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="w">    </span><span class="n">α</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">α_init</span>
<span class="w">    </span><span class="n">f_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="n">g_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="c"># Ensure p is a descent direction.</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">g_x</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="o">≥</span><span class="w"> </span><span class="mi">0</span>
<span class="w">        </span><span class="n">error</span><span class="p">(</span><span class="s">&quot;Search direction is not a descent direction.&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">α</span><span class="o">*</span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">f_x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">c</span><span class="o">*</span><span class="n">α</span><span class="o">*</span><span class="n">dot</span><span class="p">(</span><span class="n">g_x</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">)</span>
<span class="w">        </span><span class="n">α</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">τ</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">α</span>
<span class="k">end</span>

<span class="c"># Example: Steepest descent for f(x) = (x-5)^2.</span>
<span class="n">x0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span>
<span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">grad_f</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span><span class="w">  </span><span class="c"># Steepest descent direction.</span>
<span class="n">α</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">backtracking_line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">grad_f</span><span class="p">,</span><span class="w"> </span><span class="n">x0</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">)</span>
<span class="n">x1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">α</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">p</span>

<span class="n">println</span><span class="p">(</span><span class="s">&quot;Julia: x0 = &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">x0</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Julia: Step size α = &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">α</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Julia: New iterate x1 = &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">x1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div></p>
<!-- #### Python Example -->

<p>=== python</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">backtracking_line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">α_init</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">α_init</span>
    <span class="n">f_x</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">g_x</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Ensure p is a descent direction.</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">g_x</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Search direction is not a descent direction.&quot;</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">α</span> <span class="o">*</span> <span class="n">p</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">f_x</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">α</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">g_x</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="n">α</span> <span class="o">*=</span> <span class="n">τ</span>
    <span class="k">return</span> <span class="n">α</span>

<span class="c1"># Example: Steepest descent for f(x) = (x-5)^2.</span>
<span class="n">x0</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">p</span> <span class="o">=</span> <span class="o">-</span><span class="n">grad_f</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>  <span class="c1"># Steepest descent direction.</span>
<span class="n">α</span> <span class="o">=</span> <span class="n">backtracking_line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">α</span> <span class="o">*</span> <span class="n">p</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Python: x0 =&quot;</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Python: Step size α =&quot;</span><span class="p">,</span> <span class="n">α</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Python: New iterate x1 =&quot;</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<hr />
<h2 id="2-trust-region-methods">2. Trust Region Methods</h2>
<h3 id="21-concept-and-motivation">2.1. Concept and Motivation</h3>
<p>Trust region methods approach the step selection problem differently. Instead of choosing a step size along a fixed direction, these methods:
- <strong>Build a local model:</strong> Typically a quadratic model of $ f $ around $ x_k $:
  <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>m_k(p) = f(x_k) + \nabla f(x_k)^T p + \frac{1}{2} p^T B_k p,
</code></pre></div></td></tr></table></div>
  where $ B_k $ is either the Hessian <span class="arithmatex">\(\nabla^2 f(x_k)\)</span> or an approximation thereof.
- <strong>Restrict the step:</strong> Only consider steps $ p $ that lie within a neighborhood of $ x_k $ of radius <span class="arithmatex">\(\Delta_k\)</span>:
  <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\|p\| \le \Delta_k.
</code></pre></div></td></tr></table></div>
- <strong>Solve the subproblem:</strong> Find
  <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>p_k = \arg\min_{\|p\| \le \Delta_k} m_k(p).
</code></pre></div></td></tr></table></div>
- <strong>Update the iterate:</strong> Set
  <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>x_{k+1} = x_k + p_k.
</code></pre></div></td></tr></table></div></p>
<p>The idea is that the quadratic model is assumed to be a good approximation within the "trust region." If the model predicts a good decrease and the actual function decreases sufficiently, the trust region can be expanded; otherwise, it is contracted.</p>
<h3 id="22-key-components">2.2. Key Components</h3>
<ul>
<li>
<p><strong>Trust Region Subproblem:</strong><br />
  The quadratic subproblem is
  <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\min_{p} \; m_k(p) \quad \text{subject to} \quad \|p\| \le \Delta_k.
</code></pre></div></td></tr></table></div>
  This subproblem is usually solved approximately (e.g., via the dogleg method, conjugate gradient, or truncated CG).</p>
</li>
<li>
<p><strong>Trust Region Radius <span class="arithmatex">\(\Delta_k\)</span>:</strong><br />
  After computing $ p_k $, we compare the actual reduction in $ f $ to the predicted reduction by $ m_k(p) $. Based on the ratio
  <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\rho_k = \frac{f(x_k) - f(x_k+p_k)}{m_k(0) - m_k(p_k)},
</code></pre></div></td></tr></table></div>
  we adjust <span class="arithmatex">\(\Delta_k\)</span>:</p>
</li>
<li>If <span class="arithmatex">\(\rho_k\)</span> is high (model was good), increase <span class="arithmatex">\(\Delta_k\)</span>.</li>
<li>If <span class="arithmatex">\(\rho_k\)</span> is low (model was poor), decrease <span class="arithmatex">\(\Delta_k\)</span>.</li>
</ul>
<h3 id="23-code-sketch">2.3. Code Sketch</h3>
<p>Below is a simplified pseudocode outline and code snippet in Julia for a trust region step.</p>
<p><strong>Pseudocode:</strong>
1. At iterate <span class="arithmatex">\(x_k\)</span>, build a quadratic model:
   <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>m_k(p) = f(x_k) + \nabla f(x_k)^T p + \frac{1}{2} p^T B_k p.
</code></pre></div></td></tr></table></div>
2. Solve the subproblem:
   <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>p_k = \arg\min_{\|p\| \le \Delta_k} m_k(p).
</code></pre></div></td></tr></table></div>
3. Compute the ratio:
   <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\rho_k = \frac{f(x_k) - f(x_k+p_k)}{m_k(0) - m_k(p_k)}.
</code></pre></div></td></tr></table></div>
4. Update <span class="arithmatex">\(x_{k+1} = x_k + p_k\)</span> and adjust <span class="arithmatex">\(\Delta_k\)</span> based on <span class="arithmatex">\(\rho_k\)</span>.</p>
<h4 id="example-pseudocode-style">Example (Pseudocode Style)</h4>
<p>=== julia
<div class="language-julia highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">function</span><span class="w"> </span><span class="n">trust_region_step</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">grad_f</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">Δ</span><span class="p">)</span>
<span class="w">    </span><span class="c"># Define the quadratic model:</span>
<span class="w">    </span><span class="n">m</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">p</span><span class="p">)</span>

<span class="w">    </span><span class="c"># (For illustration, assume we solve the subproblem exactly; in practice, use a solver.)</span>
<span class="w">    </span><span class="c"># Here we use the dogleg method or a truncated CG method.</span>
<span class="w">    </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">solve_trust_region_subproblem</span><span class="p">(</span><span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="n">Δ</span><span class="p">)</span>

<span class="w">    </span><span class="c"># Compute actual and predicted reduction:</span>
<span class="w">    </span><span class="n">actual_reduction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">p</span><span class="p">)</span>
<span class="w">    </span><span class="n">predicted_reduction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">m</span><span class="p">(</span><span class="n">zeros</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">m</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="w">    </span><span class="n">ρ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">actual_reduction</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">predicted_reduction</span>

<span class="w">    </span><span class="c"># Update trust region radius (simplified rule):</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">ρ</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.25</span>
<span class="w">        </span><span class="n">Δ_new</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Δ</span>
<span class="w">    </span><span class="k">elseif</span><span class="w"> </span><span class="n">ρ</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.75</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">Δ</span>
<span class="w">        </span><span class="n">Δ_new</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">min</span><span class="p">(</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Δ</span><span class="p">,</span><span class="w"> </span><span class="n">Δ_max</span><span class="p">)</span><span class="w">  </span><span class="c"># assume some Δ_max is defined</span>
<span class="w">    </span><span class="k">else</span>
<span class="w">        </span><span class="n">Δ_new</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Δ</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">Δ_new</span><span class="p">,</span><span class="w"> </span><span class="n">ρ</span>
<span class="k">end</span>
</code></pre></div></td></tr></table></div></p>
<p><em>Note:</em> In an actual implementation, the subproblem solver (e.g. dogleg or truncated CG) would be used to compute <span class="arithmatex">\(p\)</span>. Packages like <a href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl">JSOSolvers.jl</a> and <a href="https://docs.sciml.ai/Optimization/">Optimization.jl</a> provide robust trust region methods.</p>
<!-- #### Python Example (Simplified) -->

<p>=== python
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">quadratic_model</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">B</span> <span class="o">@</span> <span class="n">p</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">trust_region_step</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">Δ</span><span class="p">,</span> <span class="n">Δ_max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># For illustration, assume we solve the trust region subproblem approximately.</span>
    <span class="c1"># Here we use the Cauchy point as a simple solution:</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">p_cauchy</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">Δ</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g</span><span class="p">))</span> <span class="o">*</span> <span class="n">g</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p_cauchy</span>  <span class="c1"># In practice, use dogleg or truncated CG.</span>

    <span class="n">actual_reduction</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">predicted_reduction</span> <span class="o">=</span> <span class="n">quadratic_model</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="n">quadratic_model</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">ρ</span> <span class="o">=</span> <span class="n">actual_reduction</span> <span class="o">/</span> <span class="n">predicted_reduction</span>

    <span class="k">if</span> <span class="n">ρ</span> <span class="o">&lt;</span> <span class="mf">0.25</span><span class="p">:</span>
        <span class="n">Δ_new</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">Δ</span>
    <span class="k">elif</span> <span class="n">ρ</span> <span class="o">&gt;</span> <span class="mf">0.75</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">Δ</span><span class="p">:</span>
        <span class="n">Δ_new</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">Δ</span><span class="p">,</span> <span class="n">Δ_max</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Δ_new</span> <span class="o">=</span> <span class="n">Δ</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">p</span><span class="p">,</span> <span class="n">Δ_new</span><span class="p">,</span> <span class="n">ρ</span>

<span class="c1"># Example: minimize f(x) = (x-5)^2, with constant Hessian B = 2.</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">]])</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
<span class="n">Δ</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">x1</span><span class="p">,</span> <span class="n">Δ_new</span><span class="p">,</span> <span class="n">ρ</span> <span class="o">=</span> <span class="n">trust_region_step</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">Δ</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Python: New iterate x1 =&quot;</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Python: New trust region radius Δ =&quot;</span><span class="p">,</span> <span class="n">Δ_new</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Python: Ratio ρ =&quot;</span><span class="p">,</span> <span class="n">ρ</span><span class="p">)</span>
</code></pre></div></td></tr></table></div></p>
<hr />
<h2 id="3-comparison-line-search-vs-trust-region">3. Comparison: Line Search vs. Trust Region</h2>
<ul>
<li><strong>Line Search:</strong></li>
<li><strong>Idea:</strong> Move along a fixed search direction $ p_k $ by choosing a step size <span class="arithmatex">\(\alpha_k\)</span>.</li>
<li><strong>Key Conditions:</strong> Armijo and Wolfe (or strong Wolfe) conditions.</li>
<li><strong>Pros:</strong> Often simple to implement; works well when the local model is valid along the entire line.</li>
<li>
<p><strong>Cons:</strong> Requires careful selection of <span class="arithmatex">\(\alpha_{\text{init}}\)</span> and may involve multiple function evaluations per iteration.</p>
</li>
<li>
<p><strong>Trust Region:</strong></p>
</li>
<li><strong>Idea:</strong> Build a local (often quadratic) model $ m_k(p) $ and trust that model only within a ball (trust region) of radius <span class="arithmatex">\(\Delta_k\)</span>. The step $ p_k $ is chosen by approximately minimizing $ m_k(p) $ subject to <span class="arithmatex">\(\|p\|\leq \Delta_k\)</span>.</li>
<li><strong>Pros:</strong> Naturally limits the step size when the model is unreliable; robust in the presence of nonconvexity.</li>
<li><strong>Cons:</strong> The subproblem may be harder to solve; requires mechanisms to adjust the radius <span class="arithmatex">\(\Delta_k\)</span>.</li>
</ul>
<p>Both strategies are widely used. The choice depends on the problem structure and practical considerations. In many modern solvers, hybrid strategies exist, and libraries like JSOSolvers.jl offer both line search and trust region solvers for unconstrained optimization.</p>
<hr />
<h2 id="4-further-reading-and-references">4. Further Reading and References</h2>
<p>For a deeper dive into these strategies and their analysis, consult:</p>
<ul>
<li><strong>Nocedal, Jorge, and Stephen J. Wright.</strong><br />
<em>Numerical Optimization.</em> Springer, 2006.</li>
<li><strong>Armijo, Larry.</strong> "Minimization of Functions Having Lipschitz Continuous First Partial Derivatives."<br />
<em>Pacific Journal of Mathematics,</em> 1966.</li>
<li><strong>Wolfe Conditions – Wikipedia:</strong><br />
<a href="https://en.wikipedia.org/wiki/Wolfe_conditions">https://en.wikipedia.org/wiki/Wolfe_conditions</a></li>
<li><strong>Backtracking Line Search – Wikipedia:</strong><br />
<a href="https://en.wikipedia.org/wiki/Backtracking_line_search">https://en.wikipedia.org/wiki/Backtracking_line_search</a></li>
<li><strong>JSOSolvers.jl and Optimization.jl:</strong><br />
<a href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl">JSOSolvers.jl</a><br />
<a href="https://docs.sciml.ai/Optimization/">Optimization.jl Documentation</a></li>
</ul>
<p>Additionally, explore related lecture notes on algorithms such as MINRES (for linear systems) and dedicated notes on line search strategies.</p>
<hr />
<h1 id="conclusion">Conclusion</h1>
<p>In these lecture notes, we have discussed two primary strategies for step size selection in unconstrained optimization:</p>
<ul>
<li><strong>Line Search Methods</strong> rely on finding an appropriate <span class="arithmatex">\(\alpha\)</span> along a fixed search direction using conditions such as Armijo and Wolfe conditions.  </li>
<li><strong>Trust Region Methods</strong> build a local model of the objective and restrict the step to lie within a region where that model is trustworthy.</li>
</ul>
<p>Both strategies have their theoretical merits and practical trade-offs. Modern optimization solvers often integrate these ideas (or even hybrid methods) to achieve robust and efficient performance.</p>
<p>These notes include examples in both Julia and Python to help illustrate the concepts. For further exploration, links to related topics and additional references are provided.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025. All rights reserved.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.instant", "navigation.expand", "navigation.top", "toc.integrate", "content.tabs.link", "search.highlight", "search.share"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>