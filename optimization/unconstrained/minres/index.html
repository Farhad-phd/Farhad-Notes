
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://yourwebsite.com/optimization/unconstrained/minres/">
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Comprehensive Lecture on MINRES and Newton-MR - PhD Notes: Optimization & Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Lato";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="orange" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#comprehensive-lecture-on-minres-and-newton-mr" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="PhD Notes: Optimization &amp; Machine Learning" class="md-header__button md-logo" aria-label="PhD Notes: Optimization & Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            PhD Notes: Optimization & Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Comprehensive Lecture on MINRES and Newton-MR
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="orange" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="blue-grey" data-md-color-accent="amber"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../index.md" class="md-tabs__link">
          
  
  
  Optimization

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="PhD Notes: Optimization &amp; Machine Learning" class="md-nav__button md-logo" aria-label="PhD Notes: Optimization & Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    PhD Notes: Optimization & Machine Learning
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="comprehensive-lecture-on-minres-and-newton-mr">Comprehensive Lecture on MINRES and Newton-MR</h1>
<p>It explores advanced Krylov subspace methods for symmetric systems—specifically MINRES, CG, and CR—and shows how these methods are incorporated into a Newton-type method (Newton-MR) for nonconvex smooth unconstrained optimization. Special emphasis is placed on detecting negative curvature via MINRES and the strategy to switch to a line search when such curvature is encountered.</p>
<hr />
<h2 id="1-overview-of-krylov-subspace-methods-for-symmetric-systems">1. Overview of Krylov Subspace Methods for Symmetric Systems</h2>
<p>When solving a linear system</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>Ax = b,
</code></pre></div></td></tr></table></div>
<p>iterative methods based on Krylov subspaces are preferred for large-scale or sparse problems. In symmetric settings, three key methods are:</p>
<ul>
<li>
<p><strong>Conjugate Gradient (CG):</strong><br />
  Designed for symmetric positive definite (SPD) systems. CG minimizes the energy (or <span class="arithmatex">\(A\)</span>-norm) error in the Krylov subspace. Its convergence rate is <span class="arithmatex">\(O(\sqrt{\kappa(A)})\)</span> in the best case, where <span class="arithmatex">\(\kappa(A)\)</span> is the condition number.</p>
</li>
<li>
<p><strong>Conjugate Residual (CR):</strong><br />
  Applicable to symmetric systems—including indefinite ones—CR minimizes the residual norm <span class="arithmatex">\(\|b-Ax\|\)</span> over the Krylov subspace. While it is closely related to MINRES, its recurrences require slightly more storage.</p>
</li>
<li>
<p><strong>MINRES (Minimum Residual Method):</strong><br />
  Tailored for symmetric systems (even if indefinite), MINRES directly minimizes the 2-norm of the residual
  <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>r(x) = b - Ax,
</code></pre></div></td></tr></table></div>
  by projecting the problem onto a Krylov subspace generated via the Lanczos process—a specialized Arnoldi iteration for symmetric matrices. MINRES is especially attractive because it efficiently handles negative or zero curvature situations that arise in nonconvex optimization.</p>
</li>
</ul>
<blockquote>
<p><strong>Key Remark:</strong><br />
While CG is optimal for SPD systems, MINRES extends to indefinite matrices and inherently detects directions of non-positive curvature.</p>
</blockquote>
<p><em>For additional background on MINRES, see <a href="https://doi.org/10.1137/0712047">Paige and Saunders (1975)</a> and the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Minimal_residual_method">Minimal Residual Method</a>.</em></p>
<hr />
<h2 id="2-in-depth-the-minres-method">2. In-Depth: The MINRES Method</h2>
<h3 id="21-derivation-via-the-lanczos-process">2.1. Derivation via the Lanczos Process</h3>
<p>For a symmetric matrix <span class="arithmatex">\(A\)</span>, the Arnoldi process simplifies to the Lanczos iteration. Starting with the normalized residual</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>q_1 = \frac{r_0}{\|r_0\|},
</code></pre></div></td></tr></table></div>
<p>the Lanczos process generates an orthonormal basis</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>Q_m = [q_1, q_2, \dots, q_m]
</code></pre></div></td></tr></table></div>
<p>for the Krylov subspace</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\mathcal{K}_m(A, r_0) = \operatorname{span}\{r_0, Ar_0, A^2r_0, \dots, A^{m-1}r_0\}.
</code></pre></div></td></tr></table></div>
<p>In exact arithmetic, one obtains the relation</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>A Q_m = Q_{m+1} \tilde{T}_m,
</code></pre></div></td></tr></table></div>
<p>where <span class="arithmatex">\(\tilde{T}_m\)</span> is an <span class="arithmatex">\((m+1) \times m\)</span> tridiagonal matrix. Any approximate solution in the affine subspace <span class="arithmatex">\(x_0 + \mathcal{K}_m\)</span> can then be written as</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>x_m = x_0 + Q_m y,
</code></pre></div></td></tr></table></div>
<p>with <span class="arithmatex">\(y \in \mathbb{R}^m\)</span>. The residual becomes</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>r_m = b - Ax_m = r_0 - A Q_m y = \|r_0\| \left(e_1 - \tilde{T}_m y\right),
</code></pre></div></td></tr></table></div>
<p>so that minimizing <span class="arithmatex">\(\|r_m\|\)</span> is equivalent to solving a small least-squares problem:</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>y_m = \operatorname*{argmin}_{y\in\mathbb{R}^m} \left\|\|r_0\|e_1 - \tilde{T}_m y\right\|_2.
</code></pre></div></td></tr></table></div>
<h3 id="22-convergence-properties">2.2. Convergence Properties</h3>
<p>For symmetric (even indefinite) matrices, convergence bounds for MINRES can be derived via polynomial approximation. For example, if the eigenvalues of <span class="arithmatex">\(A\)</span> are divided into positive and negative parts, one may have a bound of the form</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\frac{\|r_m\|}{\|r_0\|} \le \left(\frac{\sqrt{\kappa_+ \kappa_-} - 1}{\sqrt{\kappa_+ \kappa_-} + 1}\right)^{\lfloor m/2 \rfloor},
</code></pre></div></td></tr></table></div>
<p>where
- <span class="arithmatex">\(\kappa_+ = \frac{\max_{\lambda \in \Lambda_+}|\lambda|}{\min_{\lambda \in \Lambda_+}|\lambda|}\)</span> (for positive eigenvalues), and<br />
- <span class="arithmatex">\(\kappa_- = \frac{\max_{\lambda \in \Lambda_-}|\lambda|}{\min_{\lambda \in \Lambda_-}|\lambda|}\)</span> (for negative eigenvalues).</p>
<p>For SPD matrices, one recovers a similar bound to that for CG:</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\frac{\|r_m\|}{\|r_0\|} \le 2 \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m.
</code></pre></div></td></tr></table></div>
<p>These bounds illustrate that the convergence rate depends on the spectral properties of <span class="arithmatex">\(A\)</span>; clustering of eigenvalues near zero can slow convergence.</p>
<hr />
<h2 id="3-comparison-minres-vs-cg-vs-cr">3. Comparison: MINRES vs. CG vs. CR</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Matrix Requirements</th>
<th>Minimization Target</th>
<th>Recurrence Complexity</th>
<th>Key Strengths</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CG</strong></td>
<td>Symmetric positive definite</td>
<td>Energy norm error <span class="arithmatex">\(\|x - x^*\|_A\)</span></td>
<td>Short recurrence (<span class="arithmatex">\(O(1)\)</span> per iteration)</td>
<td>Highly efficient for SPD systems; optimal for quadratic minimization.</td>
</tr>
<tr>
<td><strong>CR</strong></td>
<td>Symmetric (SPD or indefinite)</td>
<td>2-norm residual <span class="arithmatex">\(\|b - Ax\|\)</span></td>
<td>Similar to CG with slightly more storage</td>
<td>Minimizes the residual; closely related to MINRES.</td>
</tr>
<tr>
<td><strong>MINRES</strong></td>
<td>Symmetric (including indefinite)</td>
<td>Direct minimization of <span class="arithmatex">\(\|b - Ax\|_2\)</span></td>
<td>Short recurrence via Lanczos (comparable to CG)</td>
<td>Robust for indefinite systems; inherently detects negative curvature.</td>
</tr>
</tbody>
</table>
<p>In many optimization applications—especially within a Newton-type method—the Hessian <span class="arithmatex">\(H_k\)</span> may be indefinite. In such cases, CG may not be applicable or requires modifications, whereas MINRES naturally handles these situations.</p>
<hr />
<h2 id="4-newton-mr-a-newton-type-method-for-nonconvex-optimization">4. Newton-MR: A Newton-Type Method for Nonconvex Optimization</h2>
<p>Newton-MR is a variant of Newton’s method that employs MINRES to solve the Newton system in nonconvex optimization problems. This is particularly useful when the Hessian</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>H_k = \nabla^2 f(x_k)
</code></pre></div></td></tr></table></div>
<p>is indefinite.</p>
<h3 id="41-motivation">4.1. Motivation</h3>
<ul>
<li>
<p><strong>Indefinite Hessians in Nonconvex Problems:</strong><br />
  In nonconvex settings, the Hessian can have negative or zero eigenvalues. Traditional Newton methods (or Newton-CG) may fail or require modifications such as adding a damping term, which may slow down convergence.</p>
</li>
<li>
<p><strong>Leveraging MINRES:</strong><br />
  MINRES is designed for symmetric indefinite systems. It naturally minimizes the residual norm and, via its Lanczos process, can detect directions of negative curvature. This detection is key to ensuring that the algorithm does not take a full Newton step that could lead to ascent or non-descent.</p>
</li>
</ul>
<h3 id="42-detecting-negative-or-zero-curvature-in-minres">4.2. Detecting Negative or Zero Curvature in MINRES</h3>
<p>During the MINRES iterations, the Lanczos process computes recurrence coefficients <span class="arithmatex">\(\alpha_j\)</span> (diagonal entries) and <span class="arithmatex">\(\beta_j\)</span> (off-diagonal entries) that form the tridiagonal matrix <span class="arithmatex">\(\tilde{T}_m\)</span>. These coefficients carry curvature information:</p>
<ul>
<li>
<p><strong>Rayleigh Quotient Approximation:</strong><br />
  The diagonal coefficients <span class="arithmatex">\(\alpha_j\)</span> approximate the Rayleigh quotient in the Krylov subspace. If at any iteration <span class="arithmatex">\(j\)</span> an <span class="arithmatex">\(\alpha_j\)</span> becomes non-positive (or nearly zero), it signals that the Hessian has a direction of non-positive (negative or zero) curvature.</p>
</li>
<li>
<p><strong>Practical Detection:</strong><br />
  In implementations, MINRES monitors these recurrence coefficients. Once a coefficient satisfies
  <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>\alpha_j \leq \delta \quad (\delta \approx 0),
</code></pre></div></td></tr></table></div>
  this serves as a flag that a negative curvature direction has been encountered.</p>
</li>
</ul>
<h3 id="43-switching-to-a-line-search">4.3. Switching to a Line Search</h3>
<p>When negative or zero curvature is detected, Newton-MR adopts a safeguard by switching to a line search along the direction associated with the detected curvature:</p>
<ol>
<li>
<p><strong>Extract Negative Curvature Direction:</strong><br />
   When the MINRES iteration indicates non-positive curvature, the algorithm identifies a direction <span class="arithmatex">\(d\)</span> (often related to the most recent Krylov basis vector) that exhibits negative curvature.</p>
</li>
<li>
<p><strong>Line Search Procedure:</strong><br />
   Instead of using the full Newton step <span class="arithmatex">\(p\)</span> computed via MINRES, the algorithm performs a line search along <span class="arithmatex">\(d\)</span> to ensure that the update
   <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>x_{k+1} = x_k + \alpha d
</code></pre></div></td></tr></table></div>
   yields a sufficient decrease in the objective function <span class="arithmatex">\(f\)</span>.</p>
</li>
<li>
<p><strong>Ensuring Sufficient Decrease:</strong><br />
   A common strategy is to use an Armijo condition:
   <div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code>f(x_k + \alpha d) \le f(x_k) + c \, \alpha \, g_k^T d,
</code></pre></div></td></tr></table></div>
   with <span class="arithmatex">\(c \in (0,1)\)</span>. This safeguards the descent property even when the Hessian is indefinite.</p>
</li>
<li>
<p><strong>Adaptive Strategy:</strong><br />
   The paper details an adaptive strategy whereby the MINRES subproblem is solved only until a curvature condition is violated. At that point, the algorithm “switches off” the standard Newton update and reverts to a safeguarded line search, ensuring overall descent and robust convergence.</p>
</li>
</ol>
<hr />
<h2 id="5-detailed-pseudocode-for-newton-mr-with-curvature-detection">5. Detailed Pseudocode for Newton-MR with Curvature Detection</h2>
<div class="language-markdown highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span></pre></div></td><td class="code"><div><pre><span></span><code>Algorithm Newton-MR:
Input: Initial point \(x_0\), tolerance \(\epsilon\), maximum iterations \(K\).

for \(k = 0, 1, \dots, K-1\):
<span class="w">  </span><span class="k">1.</span> Compute gradient: \(g<span class="ge">_k = \nabla f(x_</span>k)\).
<span class="w">  </span><span class="k">2.</span> If \(\|g_k\| \le \epsilon\), terminate.
<span class="w">  </span><span class="k">3.</span> Compute Hessian: \(H<span class="ge">_k = \nabla^2 f(x_</span>k)\).
<span class="w">  </span><span class="k">4.</span> Solve \(H<span class="ge">_k p = -g_</span>k\) approximately using MINRES:
     a. Start with \(p^{(0)} = 0\).
     b. Run MINRES, and at each iteration monitor the Lanczos coefficient \(\alpha_j\).
     c. If any \(\alpha_j \le \delta\) (indicating negative/zero curvature), extract the corresponding direction \(d\).
<span class="w">  </span><span class="k">5.</span> <span class="gs">**If negative curvature detected:**</span>
<span class="w">     </span><span class="k">-</span><span class="w"> </span>Switch to a safeguarded line search along \(d\) (instead of the full Newton step).
<span class="w">  </span><span class="k">6.</span> <span class="gs">**Else:**</span>
<span class="w">     </span><span class="k">-</span><span class="w"> </span>Accept the computed \(p\) as the Newton direction.
<span class="w">  </span><span class="k">7.</span> Perform a line search to determine step size \(\alpha_k\) that satisfies a sufficient decrease condition.
<span class="w">  </span><span class="k">8.</span> Update \(x<span class="ge">_{k+1} = x_</span>k + \alpha_k \times\) (chosen direction).
end for
</code></pre></div></td></tr></table></div>
<hr />
<h2 id="6-illustrative-julia-implementation">6. Illustrative Julia Implementation</h2>
<p>Below is a Julia code snippet that outlines the Newton-MR algorithm. This version includes comments on detecting negative curvature and switching to a line search. (In practice, more advanced curvature-detection logic may be integrated.)</p>
<div class="language-julia highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">using</span><span class="w"> </span><span class="n">LinearAlgebra</span><span class="p">,</span><span class="w"> </span><span class="n">IterativeSolvers</span>

<span class="s">&quot;&quot;&quot;</span>
<span class="s">    newtonMR(f, grad, hess, x0; tol, maxit)</span>

<span class="s">Newton-MR solver for unconstrained optimization.</span>
<span class="s">- `f`: objective function</span>
<span class="s">- `grad`: gradient function</span>
<span class="s">- `hess`: Hessian function</span>
<span class="s">- `x0`: initial point</span>
<span class="s">- `tol`: tolerance on the gradient norm</span>
<span class="s">- `maxit`: maximum number of outer iterations</span>
<span class="s">&quot;&quot;&quot;</span>
<span class="k">function</span><span class="w"> </span><span class="n">newtonMR</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">grad</span><span class="p">,</span><span class="w"> </span><span class="n">hess</span><span class="p">,</span><span class="w"> </span><span class="n">x0</span><span class="p">;</span><span class="w"> </span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span><span class="w"> </span><span class="n">maxit</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x0</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">1</span><span class="o">:</span><span class="n">maxit</span>
<span class="w">        </span><span class="n">g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="n">println</span><span class="p">(</span><span class="s">&quot;Iteration </span><span class="si">$k</span><span class="s">: ||grad|| = &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">g</span><span class="p">))</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">g</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tol</span>
<span class="w">            </span><span class="n">println</span><span class="p">(</span><span class="s">&quot;Convergence achieved at iteration </span><span class="si">$k</span><span class="s">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">x</span>
<span class="w">        </span><span class="k">end</span>

<span class="w">        </span><span class="n">H</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hess</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="c"># Define Hessian operator for MINRES</span>
<span class="w">        </span><span class="n">H_operator</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">H</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">v</span>

<span class="w">        </span><span class="c"># Solve H * p = -g using MINRES; monitor MINRES coefficients.</span>
<span class="w">        </span><span class="c"># (In a full implementation, one would check the recurrence coefficients here.)</span>
<span class="w">        </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">minres</span><span class="p">(</span><span class="n">H_operator</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">g</span><span class="p">,</span><span class="w"> </span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span><span class="w"> </span><span class="n">maxiter</span><span class="o">=</span><span class="n">length</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">            </span><span class="n">println</span><span class="p">(</span><span class="s">&quot;MINRES did not converge at iteration </span><span class="si">$k</span><span class="s">; flag = </span><span class="si">$flag</span><span class="s">&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="k">break</span>
<span class="w">        </span><span class="k">end</span>

<span class="w">        </span><span class="c"># Here we assume that our MINRES routine internally monitors the Lanczos recurrence.</span>
<span class="w">        </span><span class="c"># If negative curvature is detected (e.g., a coefficient falls below a threshold δ),</span>
<span class="w">        </span><span class="c"># the algorithm would set a flag and extract the corresponding direction d.</span>
<span class="w">        </span><span class="c"># For this illustrative code, we simply check a condition (dummy check):</span>
<span class="w">        </span><span class="n">negative_curvature_detected</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="w">  </span><span class="c"># Replace with actual detection logic</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">negative_curvature_detected</span>
<span class="w">            </span><span class="n">println</span><span class="p">(</span><span class="s">&quot;Negative curvature detected at iteration </span><span class="si">$k</span><span class="s">. Switching to line search.&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="c"># Assume d is the extracted negative curvature direction (here, simply use p)</span>
<span class="w">            </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">p</span><span class="w">  </span><span class="c"># In practice, d would be chosen based on the curvature information</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">p</span>
<span class="w">        </span><span class="k">end</span>

<span class="w">        </span><span class="c"># Backtracking line search using Armijo condition:</span>
<span class="w">        </span><span class="n">α</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">α</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1e-4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">α</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">g</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="p">)</span>
<span class="w">            </span><span class="n">α</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="mf">0.5</span>
<span class="w">        </span><span class="k">end</span>

<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">α</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">x</span>
<span class="k">end</span>

<span class="c"># Example: quadratic function (SPD case; for illustration)</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w">         </span><span class="c"># f(x) = 0.5 xᵀx</span>
<span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w">                    </span><span class="c"># grad f(x) = x</span>
<span class="n">hess</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">I</span><span class="w">                    </span><span class="c"># Hessian is the identity</span>

<span class="n">x0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">solution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">newtonMR</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">grad</span><span class="p">,</span><span class="w"> </span><span class="n">hess</span><span class="p">,</span><span class="w"> </span><span class="n">x0</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Solution: &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">solution</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><em>Note:</em> In real nonconvex applications, the Hessian may be indefinite. The MINRES routine would then monitor the Lanczos coefficients (<span class="arithmatex">\(\alpha_j\)</span>) to detect negative curvature (when any <span class="arithmatex">\(\alpha_j\)</span> falls below a small threshold <span class="arithmatex">\(\delta\)</span>). Upon such detection, the algorithm switches from the full Newton step to a safeguarded line search along the direction associated with the negative curvature, ensuring a sufficient decrease in <span class="arithmatex">\(f\)</span>.</p>
<hr />
<h2 id="7-concluding-remarks">7. Concluding Remarks</h2>
<ul>
<li>
<p><strong>MINRES as a Robust Subproblem Solver:</strong><br />
  MINRES directly minimizes the residual norm and is well suited for symmetric indefinite matrices. Its derivation via the Lanczos process yields efficient short recurrences and, crucially, the ability to detect directions of non-positive curvature.</p>
</li>
<li>
<p><strong>Comparative Insights:</strong><br />
  While CG is optimal for SPD systems, MINRES (and CR) extend applicability to broader classes of symmetric matrices. Their convergence properties—governed by the distribution of eigenvalues—illustrate how negative curvature can be detected and exploited.</p>
</li>
<li>
<p><strong>Newton-MR Advantages:</strong><br />
  Newton-MR leverages MINRES to solve the Newton system even when the Hessian is indefinite. When MINRES detects negative or zero curvature (via non-positive Lanczos coefficients), the algorithm switches to a line search along a negative curvature direction, ensuring descent and robust convergence. This adaptive mechanism underpins the complexity guarantees provided by the method.</p>
</li>
</ul>
<hr />
<h2 id="references">References</h2>
<ol>
<li>
<p><strong>Liu, Yang, and Fred Roosta.</strong><br />
<em>A Newton-MR Algorithm with Complexity Guarantees for Nonconvex Smooth Unconstrained Optimization.</em><br />
   ArXiv preprint, <a href="https://arxiv.org/abs/2208.07095">arXiv:2208.07095</a>. citeturn0search1</p>
</li>
<li>
<p><strong>Paige, C. C., and M. A. Saunders.</strong><br />
<em>Solution of Sparse Indefinite Systems of Linear Equations.</em><br />
   SIAM Journal on Numerical Analysis, 12 (1975): 617–629.</p>
</li>
<li>
<p><strong>Saad, Y.</strong><br />
<em>Iterative Methods for Sparse Linear Systems.</em><br />
   SIAM, 2003.</p>
</li>
<li>
<p><strong>Wikipedia.</strong><br />
<em>Minimal Residual Method.</em><br />
<a href="https://en.wikipedia.org/wiki/Minimal_residual_method">https://en.wikipedia.org/wiki/Minimal_residual_method</a> (Accessed: November 2023). citeturn0search0</p>
</li>
<li>
<p><strong>Fong, D. C.-L., and M. A. Saunders.</strong><br />
<em>MINRES: A Krylov Subspace Method for Symmetric Linear Systems.</em><br />
   (For further background on MINRES.)</p>
</li>
</ol>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025. All rights reserved.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.instant", "navigation.expand", "navigation.top", "toc.integrate", "content.tabs.link", "search.highlight", "search.share"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>