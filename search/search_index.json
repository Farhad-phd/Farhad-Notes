{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My PhD Notes","text":""},{"location":"#overview","title":"Overview","text":"<p>This website serves as a collection of notes, resources, and references for my PhD research in Optimization and Machine Learning.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Optimization</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Explore the topics in Optimization and Machine Learning to gain insights into key concepts, algorithms, and applications.</p> <p>\ud83d\ude80 Happy Learning!</p>"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/","title":"Newton MR  Yang Liu Fred Roosta","text":""},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#convergence-and-complexity-of-newton-mr","title":"Convergence and Complexity of Newton-MR","text":"<p>Lecture Notes</p>"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#1-notation-and-main-assumptions","title":"1. Notation and Main Assumptions","text":"<ul> <li>Objective: Minimize a nonconvex smooth function \\(f:\\mathbb R^n\\to\\mathbb R\\).</li> <li> <p>Gradient and Hessian:</p> </li> <li> <p>\\(g_k = \\nabla f(x_k)\\)</p> </li> <li>\\(H_k = \\nabla^2 f(x_k)\\)</li> </ul> <p>Assumptions:</p> <ol> <li>Smoothness: \\(f\\) is twice continuously differentiable.</li> <li>Lipschitz Hessian: There exists \\(L &gt; 0\\) such that for all \\(x,y\\),</li> </ol> <p>$$      |\\nabla^2 f(x) - \\nabla^2 f(y)| \\le L\\,|x - y|.    $$ 3. Bounded Hessian: There exists \\(H_{\\max}\\) such that for all iterates,</p> <p>$$      |H_k| \\le H_{\\max}.    $$</p>"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#2-algorithm-overview-newton-mr","title":"2. Algorithm Overview (Newton-MR)","text":"<p>At each iterate \\(x_k\\):</p> <ol> <li> <p>Krylov Subproblem: Call MINRES on \\(H_k s = -g_k\\), tracking:</p> </li> <li> <p>Case A (Inexact Newton): returns \\(s_k\\) with residual      \\(\\|H_k s_k + g_k\\| \\le \\eta\\,\\|g_k\\|\\), \\(0&lt;\\eta&lt;1\\).</p> </li> <li>Case B (Negative Curvature): finds \\(p_k\\) with \\(p_k^T H_k p_k &lt; 0\\).</li> <li> <p>Step Selection:</p> </li> <li> <p>If Case A, set \\(s = s_k\\).</p> </li> <li>If Case B, set \\(s = \\alpha\\,p_k\\) for a suitable \\(\\alpha&gt;0\\).</li> <li>Line-Search: Choose \\(\\alpha_k\\) satisfying sufficient decrease.</li> <li>Update: \\(x_{k+1} = x_k + \\alpha_k s.\\)</li> </ol>"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#3-descent-lemmas","title":"3. Descent Lemmas","text":""},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#31-case-a-inexact-newton-step","title":"3.1 Case A: Inexact Newton Step","text":"<p>Using Taylor expansion with remainder (Lipschitz Hessian):</p> \\[   f(x_k + s_k) \\le f(x_k) + g_k^T s_k + \\tfrac12 s_k^T H_k s_k + \\tfrac{L}{6}\\|s_k\\|^3. \\] <p>Define \\(r_k = H_k s_k + g_k\\), \\(\\|r_k\\| \\le \\eta\\|g_k\\|\\).  Then:</p> \\[   \\Delta f_k = f(x_k)-f(x_k+s_k)   \\ge \\tfrac12 s_k^T H_k s_k - \\|s_k\\|\\|r_k\\| - \\tfrac{L}{6}\\|s_k\\|^3. \\] <p>Bounding eigenvalues and choosing \\(\\eta\\) small yields:</p> \\[   \\Delta f_k \\ge c_1\\,\\frac{\\|g_k\\|^{3/2}}{\\sqrt{H_{\\max}}}, \\] <p>for some constant \\(c_1&gt;0\\).</p>"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#32-case-b-negative-curvature-step","title":"3.2 Case B: Negative-Curvature Step","text":"<p>Suppose \\(\\mu_k = -p_k^T H_k p_k &gt; 0, \\|p_k\\|=1\\).  For step \\(s=\\alpha p_k\\) and optimal \\(\\alpha\\), the cubic model gives:</p> \\[   \\Delta f_k \\ge \\frac{2}{3\\sqrt{L}}\\,\\mu_k^{3/2}. \\] <p>Thus any negative-curvature direction yields \\(\\Omega(\\mu_k^{3/2})\\) decrease.</p>"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#4-complexity-analysis","title":"4. Complexity Analysis","text":"<p>Let \\(f^* = \\inf_x f(x)\\).  Summing \\(\\Delta f_k\\) over \\(K\\) iterations,</p> \\[   f(x_0) - f^* \\ge \\sum_{k=0}^{K-1} \\Delta f_k. \\]"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#41-first-order-stationarity-gradient","title":"4.1 First-Order Stationarity (Gradient)","text":"<p>Goal: find \\(\\min_k \\|g_k\\| \\le \\varepsilon\\).</p> <ul> <li>Each Case A step with \\(\\|g_k\\|&gt;\\varepsilon\\) decreases \\(f\\) by at least \\(c_1\\,\\varepsilon^{3/2}/\\sqrt{H_{\\max}}\\).</li> <li>Hence at most \\(\\tfrac{f(x_0)-f^*}{c_1}\\,(H_{\\max})^{1/2}\\,\\varepsilon^{-3/2}\\) such steps.</li> <li>Negative-curvature steps only help further.</li> </ul> <p>Iteration Complexity: \\(O(\\varepsilon^{-3/2}).\\)</p>"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#42-second-order-stationarity-hessian","title":"4.2 Second-Order Stationarity (Hessian)","text":"<p>Goal: ensure \\(\\|g_k\\|\\le \\varepsilon\\) and smallest eigenvalue \\(\\lambda_{\\min}(H_k) \\ge -\\varepsilon\\).</p> <ul> <li>Negative-curvature steps (Case B) with \\(\\mu_k&gt;\\varepsilon\\) each yield at least \\(C_2\\,\\varepsilon^{3/2}\\) decrease.</li> <li>Counting both gradient-large and curvature-large phases gives \\(O(\\varepsilon^{-3})\\) total iterations.</li> </ul> <p>Iteration Complexity: \\(O(\\varepsilon^{-3}).\\)</p>"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#5-per-iteration-cost-and-overall-complexity","title":"5. Per-Iteration Cost and Overall Complexity","text":"<ul> <li>Per Iteration: one MINRES call (cost \u2248 # of Hessian-vector products).  MINRES both solves and detects curvature.</li> <li>Total Hessian-Vector Products: proportional to iteration count.</li> </ul> Guarantee Iteration Count Work per Iteration 1st-Order Stationary \\(O(\\varepsilon^{-3/2})\\) MINRES (\\~matrix\u2013vector) 2nd-Order Stationary \\(O(\\varepsilon^{-3})\\) MINRES (\\~matrix\u2013vector)"},{"location":"Papers/MINRES/Newton-MR%20_Yang_Liu_Fred_Roosta/#6-summary","title":"6. Summary","text":"<p>Newton-MR unifies Newton and negative-curvature steps via a single MINRES subsolver, achieving:</p> <ul> <li>Fast local descent: \\(O(\\|g\\|^{3/2})\\) or \\(O(\\mu^{3/2})\\) per iteration.</li> <li>Optimal worst-case rates: matching cubic-regularization and trust-region methods, but simpler to implement.</li> </ul> <p>These notes provide the key inequalities and counting arguments you need to understand the convergence and complexity proofs of Newton-MR.</p>"},{"location":"optimization/Rate_Convergance_vs_complexity/","title":"Rate Convergance vs complexity","text":"<p>Overview</p> <p>In iterative algorithms\u2014particularly those for solving equations or optimization problems\u2014we care about two related but distinct measures of performance:</p> <ol> <li>Complexity: How many computational resources (time, flops, memory) does the algorithm use as a function of problem size or tolerance?</li> <li>Rate of convergence: How quickly does the sequence of iterates \\(\\{x_k\\}\\) produced by the algorithm approach the exact solution \\(x^*\\)?</li> </ol> <p>We\u2019ll first define and illustrate these notions in general.  Then we\u2019ll work through three concrete methods\u2014Newton\u2019s method, a (local) trust-region method, and the MINRES Krylov solver\u2014showing how to establish their convergence rates and bounds.</p>"},{"location":"optimization/Rate_Convergance_vs_complexity/#1-complexity-vs-convergence-rate","title":"1. Complexity vs. Convergence Rate","text":""},{"location":"optimization/Rate_Convergance_vs_complexity/#11-complexity","title":"1.1 Complexity","text":"<ul> <li>Time complexity (iterative\u2010solver perspective): If an algorithm produces an iterate \\(x_k\\) that satisfies \\(\\|x_k - x^*\\|\\le \\varepsilon\\), define</li> </ul> <p>$$     N(\\varepsilon)\\;=\\;\\min{\\,k:|x_k - x^*|\\le\\varepsilon}.   $$</p> <p>We ask: How does \\(N(\\varepsilon)\\) scale as \\(\\varepsilon\\to0\\)?  Typical answers:</p> <ul> <li>Polynomial: \\(N(\\varepsilon)=\\mathcal O(\\varepsilon^{-p})\\).</li> <li>Logarithmic: \\(N(\\varepsilon)=\\mathcal O(\\ln(1/\\varepsilon))\\).</li> <li> <p>Power\u2010log: e.g.\\ \\(\\mathcal O(\\ln\\ln(1/\\varepsilon))\\), etc.</p> </li> <li> <p>Computational cost: Multiply \\(N(\\varepsilon)\\) by the cost per iteration (e.g.\\ cost to factor or apply a matrix, line\u2010search cost, etc.).</p> </li> </ul>"},{"location":"optimization/Rate_Convergance_vs_complexity/#12-rate-of-convergence","title":"1.2 Rate of Convergence","text":"<p>Let \\(\\{x_k\\}\\) be a sequence that converges to \\(x^*\\).  We classify:</p> <ol> <li>Linear convergence: there exists \\(0&lt;\\rho&lt;1\\) and \\(C&gt;0\\) such that</li> </ol> <p>$$      |x_{k+1}-x^*|\\;\\le\\; C\\,\\rho^k\\,.    $$</p> <p>Equivalently, eventually</p> <p>$$      |x_{k+1}-x^|\\;\\le\\;\\rho\\,|x_k - x^|\\,.    $$</p> <ol> <li>Superlinear convergence:</li> </ol> <p>$$      \\lim_{k\\to\\infty}\\frac{|x_{k+1}-x^|}{|x_k - x^|}=0.    $$</p> <p>Means error ratio tends to zero.</p> <ol> <li>Quadratic convergence: there is \\(M&gt;0\\) such that, for all sufficiently large \\(k\\),</li> </ol> <p>$$      |x_{k+1}-x^|\\;\\le\\;M\\,|x_k - x^|^2.    $$</p> <ol> <li>Order-\\(p\\) convergence: if</li> </ol> <p>$$      |x_{k+1}-x^|\\;\\le\\;C\\,|x_k - x^|^p,    $$</p> <p>for some \\(p&gt;1\\), we say convergence is of order \\(p\\).</p>"},{"location":"optimization/Rate_Convergance_vs_complexity/#2-how-to-prove-a-rate-and-find-bounds","title":"2. How to Prove a Rate and Find Bounds","text":"<ol> <li>Model the error recurrence.  Write the error \\(e_k = x_k - x^*\\) and derive an inequality of the form</li> </ol> <p>$$      |e_{k+1}|\\;\\le\\;M\\,|e_k|^p,    $$</p> <p>valid in a neighborhood of \\(x^*\\).  This often uses Taylor expansions or spectral bounds.</p> <ol> <li> <p>Identify constants.  Carefully bound higher\u2010order terms to find explicit \\(M\\) and radius of convergence.</p> </li> <li> <p>Conclude order.  If \\(p=1\\) with a constant factor \\(&lt;1\\), linear; if \\(p=2\\), quadratic; in general order-\\(p\\).</p> </li> <li> <p>Translate to iteration count.  Solve \\(\\|e_k\\|\\approx C(\\|e_0\\|)^{p^k}\\) or invert the recurrence to get \\(k\\) in terms of tolerance \\(\\varepsilon\\).</p> </li> </ol>"},{"location":"optimization/Rate_Convergance_vs_complexity/#3-example-1-newtons-method-for-scalar-root-finding","title":"3. Example 1: Newton\u2019s Method for Scalar Root-Finding","text":"<p>Problem.  Solve \\(f(x)=0\\), with \\(f:\\mathbb R\\to\\mathbb R\\), \\(f\\in C^2\\), and assume \\(f'(x^*)\\neq0\\).</p> <p>Iteration</p> \\[   x_{k+1} \\;=\\; x_k \\;-\\;\\frac{f(x_k)}{f'(x_k)}. \\] <p>Error analysis.  Let \\(e_k=x_k-x^*\\).  Taylor\u2010expand \\(f\\) around \\(x^*\\):</p> \\[   f(x_k) = f'(x^*)\\,e_k + \\tfrac12f''(\\xi_k)e_k^2,   \\quad   f'(x_k)=f'(x^*)+\\tfrac12f''(\\eta_k)e_k. \\] <p>Then one shows (for small \\(e_k\\))</p> \\[   e_{k+1}   = -\\,\\frac{ \\tfrac12f''(\\xi_k)e_k^2 }{f'(x^*) + O(e_k)}   = -\\,\\frac{f''(\\xi_k)}{2 f'(x^*)} \\,e_k^2 + O(e_k^3). \\] <p>Hence, there exists \\(M\\) so that</p> \\[   |e_{k+1}|\\;\\le\\;M\\,|e_k|^2, \\] <p>i.e.\\ quadratic convergence.  Convergence is local: requires \\(x_0\\) sufficiently near \\(x^*\\).  One can bound the region of convergence by ensuring \\(\\|e_k\\|\\) stays small enough that denominators don\u2019t vanish.</p> <p>Iteration count.</p> \\[   |e_k|\\;\\approx\\;M^{\\,2^k-1}\\,|e_0|^{2^k}, \\] <p>so to achieve \\(|e_k|\\le\\varepsilon\\) one needs only \\(\\displaystyle k\\;\\approx\\;\\log_2\\!\\bigl(\\log_{1/|e_0|}(1/\\varepsilon)\\bigr)\\), which is extremely fast.</p>"},{"location":"optimization/Rate_Convergance_vs_complexity/#4-example-2-local-trust-region-method-for-optimization","title":"4. Example 2: (Local) Trust-Region Method for Optimization","text":"<p>Problem.  Minimize \\(f:\\mathbb R^n\\to\\mathbb R\\), twice continuously differentiable, with \\(\\nabla^2 f(x^*)\\) positive definite at a local minimizer \\(x^*\\).</p> <p>Algorithm sketch.  At iterate \\(x_k\\), build quadratic model</p> \\[   m_k(s) = f(x_k) + \\nabla f(x_k)^Ts + \\tfrac12 s^T \\nabla^2 f(x_k)\\,s, \\] <p>and solve \\(\\min_{\\|s\\|\\le\\Delta_k} m_k(s)\\) to obtain step \\(s_k\\).  Update \\(x_{k+1}=x_k+s_k\\), adjust \\(\\Delta_k\\) by trust\u2010region ratio.</p> <p>Local convergence rate.  Under standard assumptions (Lipschitz Hessian near \\(x^*\\), initial \\(\\Delta_0\\) small, steps accepted), one shows that once \\(x_k\\) is close enough:</p> <ul> <li>The model is a good approximation:   \\(\\|s_k - (-[\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k))\\| = O(\\|\\nabla f(x_k)\\|)\\).</li> <li>Since Newton\u2019s step has quadratic convergence, the trust-region steps inherit quadratic convergence:</li> </ul> <p>$$     |x_{k+1}-x^|\\;\\le\\;C\\,|x_k-x^|^2,   $$</p> <p>for some \\(C\\), once \\(k\\) is large.</p> <p>One proves the bound by combining the model\u2010error (third\u2010order Taylor terms) and the fact that for small gradient, the trust\u2010region constraint is inactive (so one takes the full Newton step).</p>"},{"location":"optimization/Rate_Convergance_vs_complexity/#5-example-3-minres-for-symmetric-linear-systems","title":"5. Example 3: MINRES for Symmetric Linear Systems","text":"<p>Problem.  Solve \\(A x = b\\) with \\(A=A^T\\) (possibly indefinite), using MINRES.</p> <p>Convergence theory (simplified).  MINRES finds \\(x_k\\) in the \\(k\\)th Krylov subspace \\(\\mathcal K_k(A,r_0)\\) minimizing the residual norm \\(\\|b - A x_k\\|\\).  One can show (via polynomial approximation theory) that</p> \\[   \\frac{\\|r_k\\|}{\\|r_0\\|}    = \\min_{p\\in \\mathcal P_k,\\,p(0)=1} \\max_{\\lambda\\in\\mathrm{spec}(A)} |p(\\lambda)|, \\] <p>where \\(\\mathcal P_k\\) are polynomials of degree \\(\\le k\\).  If \\(A\\) is positive definite with spectrum in \\([\\lambda_{\\min},\\lambda_{\\max}]\\), then the optimal bound uses Chebyshev polynomials to yield linear (actually geometric) convergence:</p> \\[   \\frac{\\|r_k\\|}{\\|r_0\\|}    \\;\\le\\;2\\!\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k,   \\quad \\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}. \\] <p>Thus MINRES converges linearly with factor \\(\\rho = (\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1)\\) less than 1.  If \\(A\\) has both positive and negative eigenvalues, a similar bound holds on the union of intervals, yielding the same asymptotic rate in \\(\\sqrt{\\kappa}\\), where \\(\\kappa\\) is the ratio of largest to smallest magnitude eigenvalues.</p>"},{"location":"optimization/Rate_Convergance_vs_complexity/#6-summary","title":"6. Summary","text":"Method Convergence Rate Key bound Newton (scalar) Quadratic (\\(p=2\\)) \\(\\|e_{k+1}\\|\\le M\\|e_k\\|^2\\) Trust\u2010region Locally quadratic \\(\\|x_{k+1}-x^*\\|\\le C\\|x_k-x^*\\|^2\\) MINRES Linear (geometric) \\(\\|r_k\\|\\le2\\bigl(\\tfrac{\\sqrt\\kappa-1}{\\sqrt\\kappa+1}\\bigr)^k\\|r_0\\|\\) <ul> <li>Quadratic methods (Newton, trust\u2010region) yield extremely rapid local convergence at the cost of per\u2010iteration Hessian factorization or model solution.</li> <li>Krylov methods (MINRES) are matrix\u2010vector based, cheap per iteration, but only linear\u2014though with a condition\u2010number\u2010dependent rate.</li> </ul>"},{"location":"optimization/Rate_Convergance_vs_complexity/#takehome-messages-for-undergraduates","title":"Take\u2010Home Messages for Undergraduates","text":"<ol> <li>Distinguish complexity (iterations\u2009\u00d7\u2009cost per iteration) from convergence rate (how error shrinks each iteration).</li> <li>Classify convergence by comparing \\(\\|e_{k+1}\\|\\) to \\(\\|e_k\\|\\) via constants and exponents.</li> <li>Use Taylor expansions or polynomial\u2010approximation arguments to derive the error recurrence.</li> <li>Translate local error recurrences into global statements about \\(N(\\varepsilon)\\) vs.\\ \\(\\varepsilon\\).</li> <li>Balance high per\u2010step cost and fewer steps (Newton/trust\u2010region) against cheap steps and more iterations (Krylov/CG/MINRES).</li> </ol>"},{"location":"optimization/set_of_iteration/","title":"Set of iteration","text":"<p>Note on Splitting Iterations into \\(\\mathcal{T}_k^\\tau\\) and \\(\\mathcal{W}_k^\\tau\\)</p>"},{"location":"optimization/set_of_iteration/#1-definitions","title":"1. Definitions","text":"<p>Let \\(k_0\\) be the initial iteration index (often \\(1\\)), and \\(k\\) the current iteration.</p> <ol> <li> <p>Successful vs. Unsuccessful Iterations</p> </li> <li> <p>\\(\\displaystyle \\mathcal{S}   = \\{\\,i\\in\\mathbb{N} \\mid \\rho_i \\ge \\eta_1\\}\\)      All successful iterations (those with ratio \\(\\rho_i\\) above threshold \\(\\eta_1\\)).</p> </li> <li> <p>\\(\\displaystyle \\mathcal{U}   = \\{\\,i\\in\\mathbb{N} \\mid \\rho_i &lt; \\eta_1\\}\\)      All unsuccessful iterations.</p> </li> <li> <p>Iterations up to \\(k\\)</p> </li> <li> <p>\\(\\displaystyle \\mathcal{S}_k = \\{\\,i\\in\\mathcal{S} : i \\le k\\}\\)      Successful iterations up to and including \\(k\\).</p> </li> <li> <p>\\(\\displaystyle \\mathcal{U}_k = \\{\\,i\\in\\mathcal{U} : i \\le k\\}\\)      Unsuccessful iterations up to and including \\(k\\).</p> </li> <li> <p>Cardinality \\(\\lvert \\mathcal{S}_j\\rvert\\) denotes the number of elements in the set \\(\\mathcal{S}_j\\), i.e.</p> </li> </ol> <p>$$      \\lvert\\mathcal{S}_j\\rvert \\;=\\;#{\\,i\\le j : i\\in\\mathcal{S}}.    $$</p> <ol> <li>Budget Threshold    For a parameter \\(\\tau&gt;0\\), define at each iteration \\(j\\) the \u201cbudget\u201d</li> </ol> <p>$$      B_j \\;=\\;\\tau\\,\\lvert\\mathcal{S}_j\\rvert.    $$</p> <ol> <li>Splitting    For \\(j = k_0,\\,k_0+1,\\,\\dots,k\\), partition into two sets:</li> </ol> <p>$$    \\begin{aligned}      \\mathcal{T}_k^\\tau         &amp;= \\bigl{\\,j \\mid j_0\\le j \\le k,\\;j \\le B_j \\bigr},         &amp;&amp;\\text{(\u201ctame\u201d / on\u2010budget)}\\      \\mathcal{W}_k^\\tau         &amp;= \\bigl{\\,j \\mid j_0\\le j \\le k,\\;j &gt; B_j \\bigr},         &amp;&amp;\\text{(\u201cwild\u201d / over\u2010budget)}    \\end{aligned}    $$</p>"},{"location":"optimization/set_of_iteration/#2-toy-example","title":"2. Toy Example","text":"<ul> <li> <p>Setup</p> </li> <li> <p>\\(k_0 = 1,\\;k = 6\\)</p> </li> <li>Success threshold \\(\\eta_1\\) yields successes at iterations \\(\\{1,3,4,6\\}\\).</li> <li>Thus \\(\\mathcal{S} = \\{1,3,4,6\\}\\), \\(\\mathcal{U} = \\{2,5\\}\\).</li> <li> <p>Choose \\(\\tau = 1.5\\).</p> </li> <li> <p>Compute for each \\(j=1,\\dots,6\\):</p> </li> <li> <p>\\(\\lvert\\mathcal{S}_j\\rvert\\)</p> </li> <li>Budget \\(B_j = \\tau\\,\\lvert\\mathcal{S}_j\\rvert\\)</li> <li>Check if \\(j \\le B_j\\)</li> </ul> \\(j\\) \\(\\lvert\\mathcal S_j\\rvert\\) \\(B_j = \\tau\\,\\lvert\\mathcal S_j\\rvert\\) \\(j \\le B_j\\)? Classification 1 1 1.5 \u2714\ufe0f Yes \\(\\mathcal T_6^{1.5}\\) 2 1 1.5 \u274c No \\(\\mathcal W_6^{1.5}\\) 3 2 3.0 \u2714\ufe0f Yes \\(\\mathcal T_6^{1.5}\\) 4 3 4.5 \u2714\ufe0f Yes \\(\\mathcal T_6^{1.5}\\) 5 3 4.5 \u274c No \\(\\mathcal W_6^{1.5}\\) 6 4 6.0 \u2714\ufe0f Yes \\(\\mathcal T_6^{1.5}\\) <ul> <li>Resulting Sets</li> </ul> <p>$$     \\mathcal T_6^{1.5} = {1,3,4,6},      \\quad     \\mathcal W_6^{1.5} = {2,5}.   $$</p>"},{"location":"optimization/set_of_iteration/#3-why-this-matters","title":"3. Why This Matters","text":"<ul> <li>Bounding Iterations. By separating on-budget vs. over-budget steps, one can show there cannot be too many \u201cwild\u201d iterations without generating more successes (i.e.\\ growing \\(\\lvert\\mathcal S_j\\rvert\\)) or violating some global bound.</li> <li>Complexity Analysis. This partition is key in deriving worst-case iteration counts for adaptive algorithms (trust-region, line-search, etc.), ensuring convergence within a finite number of steps.</li> </ul>"},{"location":"optimization/unconstrained/Intro/","title":"Introduction to Unconstrained Optimization","text":"<p>In unconstrained optimization, we aim to find a point <pre><code>x^* \\in \\mathbb{R}^n\n</code></pre> that minimizes a continuously differentiable function <pre><code>\\min_{x \\in \\mathbb{R}^n} f(x).\n</code></pre> That is, we seek \\(x^*\\) such that <pre><code>f(x^*) \\le f(x),\\quad \\forall\\, x \\in \\mathbb{R}^n.\n</code></pre> This framework underlies many algorithms in numerical optimization and forms the basis for further generalizations to constrained problems.</p>"},{"location":"optimization/unconstrained/Intro/#1-problem-statement","title":"1. Problem Statement","text":"<p>We consider the general unconstrained optimization problem: <pre><code>\\min_{x \\in \\mathbb{R}^n} f(x),\n</code></pre> where \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is assumed to be continuously differentiable (and, in many cases, twice continuously differentiable).</p> <p>For example, a simple quadratic problem is <pre><code>f(x) = \\frac{1}{2}x^T Q x - b^T x,\n</code></pre> with \\(Q \\in \\mathbb{R}^{n \\times n}\\) symmetric and positive definite and \\(b \\in \\mathbb{R}^n\\).</p>"},{"location":"optimization/unconstrained/Intro/#2-taylor-expansion","title":"2. Taylor Expansion","text":"<p>To analyze and approximate the behavior of \\(f\\) near a given point \\(x_k\\), we use the Taylor expansion. For a twice continuously differentiable function, the second-order Taylor expansion about \\(x_k\\) is <pre><code>f(x_k + p) \\approx f(x_k) + \\nabla f(x_k)^T p + \\frac{1}{2} p^T \\nabla^2 f(x_k) p,\n</code></pre> where: - \\(\\nabla f(x_k)\\) is the gradient at \\(x_k\\), - \\(\\nabla^2 f(x_k)\\) is the Hessian matrix at \\(x_k\\), - \\(p\\) is a perturbation (or search) direction.</p>"},{"location":"optimization/unconstrained/Intro/#21-why-use-taylor-expansion","title":"2.1. Why Use Taylor Expansion?","text":"<p>The Taylor expansion is used for several key reasons: - Local Approximation: It provides a quadratic model of \\(f\\) near \\(x_k\\) that is easier to minimize than the full nonlinear function. - Derivation of Optimality Conditions: The first-order term leads directly to the necessary condition for optimality: \\(\\nabla f(x^*) = 0\\). - Algorithm Design: Many iterative methods (such as Newton\u2019s method and quasi-Newton methods) use the quadratic model to compute search directions.</p>"},{"location":"optimization/unconstrained/Intro/#3-optimality-conditions","title":"3. Optimality Conditions","text":"<p>Optimality conditions are necessary (and sometimes sufficient) for a point to be a minimizer.</p>"},{"location":"optimization/unconstrained/Intro/#31-first-order-optimality-condition","title":"3.1. First-Order Optimality Condition","text":"<p>For a differentiable function \\(f\\), if \\(x^*\\) is a local minimizer, then <pre><code>\\nabla f(x^*) = 0.\n</code></pre> This is a first-order necessary condition.</p>"},{"location":"optimization/unconstrained/Intro/#32-second-order-optimality-conditions","title":"3.2. Second-Order Optimality Conditions","text":"<p>If \\(f\\) is twice continuously differentiable, then in addition to \\(\\nabla f(x^*) = 0\\), a local minimizer must satisfy: <pre><code>\\nabla^2 f(x^*) \\succeq 0,\n</code></pre> meaning that the Hessian is positive semidefinite at \\(x^*\\). If \\(\\nabla^2 f(x^*) \\succ 0\\) (positive definite), then \\(x^*\\) is a strict local minimizer.</p>"},{"location":"optimization/unconstrained/Intro/#4-local-vs-global-optima","title":"4. Local vs. Global Optima","text":"<ul> <li>Local Minimizer: A point \\(x^*\\) is a local minimizer if there exists a neighborhood \\(U\\) such that   <pre><code>f(x^*) \\le f(x), \\quad \\forall\\, x \\in U.\n</code></pre></li> <li>Global Minimizer: A point \\(x^*\\) is a global minimizer if   <pre><code>f(x^*) \\le f(x), \\quad \\forall\\, x \\in \\mathbb{R}^n.\n</code></pre></li> </ul> <p>For convex functions (see next section), every local minimizer is global. For nonconvex functions, however, multiple local minima may exist.</p>"},{"location":"optimization/unconstrained/Intro/#5-convex-and-concave-functions","title":"5. Convex and Concave Functions","text":"<ul> <li> <p>Convex Function:   A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x, y \\in \\mathbb{R}^n\\) and for all \\(\\theta \\in [0,1]\\),   <pre><code>f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta)f(y).\n</code></pre>   In convex optimization, any local minimizer is also a global minimizer.</p> </li> <li> <p>Concave Function:   A function is concave if \\(-f\\) is convex. That is,   <pre><code>f(\\theta x + (1-\\theta)y) \\ge \\theta f(x) + (1-\\theta)f(y).\n</code></pre></p> </li> </ul> <p>Convexity plays a central role in the design and analysis of optimization algorithms. For instance, many algorithms (gradient descent, Newton\u2019s method) have stronger convergence guarantees when applied to convex problems.</p>"},{"location":"optimization/unconstrained/Intro/#6-examples-in-julia-and-python","title":"6. Examples in Julia and Python","text":"<p>Below are simple examples of solving a one-dimensional unconstrained optimization problem using steepest descent with a backtracking line search. The same example is shown in both Julia and Python.</p>"},{"location":"optimization/unconstrained/Intro/#61-example-problem","title":"6.1. Example Problem","text":"<p>Consider the function <pre><code>f(x) = (x-5)^2,\n</code></pre> which is convex with a unique global minimizer at \\(x^*=5\\).</p>"},{"location":"optimization/unconstrained/Intro/#62-example","title":"6.2. Example","text":"julia <pre><code># Define the objective function and its gradient\nf(x) = (x - 5)^2\ngrad_f(x) = 2*(x - 5)\n\n# Backtracking line search function\nfunction backtracking_line_search(f, grad_f, x, p; \u03b1_init=1.0, \u03c4=0.5, c=1e-4)\n    \u03b1 = \u03b1_init\n    f_x = f(x)\n    g_x = grad_f(x)\n    # Ensure p is a descent direction\n    if dot(g_x, p) \u2265 0\n        error(\"Search direction is not a descent direction.\")\n    end\n    while f(x + \u03b1*p) &gt; f_x + c*\u03b1*dot(g_x, p)\n        \u03b1 *= \u03c4\n    end\n    return \u03b1\nend\n\n# Steepest descent iteration (single step)\nx0 = 0.0\np = -grad_f(x0)  # steepest descent direction\n\u03b1 = backtracking_line_search(f, grad_f, x0, p)\nx1 = x0 + \u03b1*p\n\nprintln(\"Julia: x0 = \", x0)\nprintln(\"Julia: Step size \u03b1 = \", \u03b1)\nprintln(\"Julia: New iterate x1 = \", x1)\n</code></pre> python <pre><code>import numpy as np\n\ndef f(x):\n    return (x - 5)**2\n\ndef grad_f(x):\n    return 2*(x - 5)\n\ndef backtracking_line_search(f, grad_f, x, p, \u03b1_init=1.0, \u03c4=0.5, c=1e-4):\n    \u03b1 = \u03b1_init\n    f_x = f(x)\n    g_x = grad_f(x)\n    # Ensure p is a descent direction\n    if np.dot(g_x, p) &gt;= 0:\n        raise ValueError(\"Search direction is not a descent direction.\")\n    while f(x + \u03b1 * p) &gt; f_x + c * \u03b1 * np.dot(g_x, p):\n        \u03b1 *= \u03c4\n    return \u03b1\n\n# Steepest descent iteration (single step)\nx0 = 0.0\np = -grad_f(x0)  # steepest descent direction\n\u03b1 = backtracking_line_search(f, grad_f, x0, p)\nx1 = x0 + \u03b1 * p\n\nprint(\"Python: x0 =\", x0)\nprint(\"Python: Step size \u03b1 =\", \u03b1)\nprint(\"Python: New iterate x1 =\", x1)\n</code></pre>"},{"location":"optimization/unconstrained/Intro/#7-additional-topics-and-further-reading","title":"7. Additional Topics and Further Reading","text":"<p>For further study, you are encouraged to explore:</p> <ul> <li> <p>MINRES and Other Krylov Methods:   These methods are used for solving large-scale linear systems and appear in second-order methods such as Newton\u2013MR. Learn more about MINRES (link to a dedicated MINRES lecture note)</p> </li> <li> <p>Line Search Techniques:   Inexact line search methods (including the Wolfe and strong Wolfe conditions) are central to ensuring robust convergence in many algorithms. Learn more about Line Search (link to a dedicated line search lecture note)</p> </li> <li> <p>Global vs. Local Optimality:   Understanding when a local minimum is also global (e.g. in convex problems) is crucial for algorithm design and analysis.</p> </li> <li> <p>Convexity and Concavity:   These properties not only affect the quality of solutions but also the convergence guarantees of optimization algorithms.</p> </li> <li> <p>Taylor Expansion and Its Role in Optimization:   The quadratic approximation provided by Taylor\u2019s expansion underpins many optimization methods.</p> </li> </ul>"},{"location":"optimization/unconstrained/Intro/#references","title":"References","text":"<ol> <li>Nocedal, Jorge, and Stephen J. Wright. Numerical Optimization. Springer, 2006.</li> <li>Armijo, Larry.    \"Minimization of Functions Having Lipschitz Continuous First Partial Derivatives.\" Pacific Journal of Mathematics, 1966.</li> <li>Wolfe Conditions \u2013 Wikipedia. https://en.wikipedia.org/wiki/Wolfe_conditions</li> <li>Backtracking Line Search \u2013 Wikipedia. https://en.wikipedia.org/wiki/Backtracking_line_search</li> <li>LineSearches.jl Documentation. https://github.com/JuliaNLSolvers/LineSearches.jl</li> <li>JSOSolvers.jl. https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl</li> </ol>"},{"location":"optimization/unconstrained/line_vs_trust/","title":"Two Strategies in Unconstrained Optimization: Line Search vs. Trust Region","text":"<p>In iterative optimization, after determining a search direction $ p_k $ from the current iterate $ x_k $, one must decide how far to move along that direction. Two broad strategies to determine the step are:</p> <ol> <li> <p>Line Search Methods:    Find a step size \\(\\alpha_k\\) such that the new iterate is    <pre><code>x_{k+1} = x_k + \\alpha_k p_k,\n</code></pre>    and \\(\\alpha_k\\) is chosen to guarantee sufficient decrease (or satisfy other conditions).</p> </li> <li> <p>Trust Region Methods:    Instead of moving along a fixed direction with a step size, these methods first build a local model $ m_k(p) $ of the objective function around $ x_k $ and then solve a subproblem that restricts the step $ p $ to lie within a neighborhood (trust region) of $ x_k $. The update is then    <pre><code>x_{k+1} = x_k + p_k, \\quad \\text{with } p_k = \\arg\\min_{\\|p\\|\\le \\Delta_k} m_k(p),\n</code></pre>    where \\(\\Delta_k\\) is the trust-region radius.</p> </li> </ol> <p>Below we explore each strategy in more detail.</p>"},{"location":"optimization/unconstrained/line_vs_trust/#1-line-search-methods","title":"1. Line Search Methods","text":""},{"location":"optimization/unconstrained/line_vs_trust/#11-concept-and-motivation","title":"1.1. Concept and Motivation","text":"<p>In a line search method, given a descent direction $ p_k $, we define the one-dimensional function <pre><code>\\phi(\\alpha) = f(x_k + \\alpha p_k),\n</code></pre> and seek a step length \\(\\alpha_k\\) that (approximately) minimizes \\(\\phi\\). However, rather than computing the exact minimizer, inexact line search methods impose conditions to guarantee sufficient progress. A common requirement is the Armijo condition: <pre><code>f(x_k + \\alpha_k p_k) \\le f(x_k) + c \\alpha_k \\nabla f(x_k)^T p_k,\n</code></pre> with a small constant \\(c \\in (0,1)\\) (often \\(c \\approx 10^{-4}\\)).</p> <p>To further prevent \\(\\alpha_k\\) from being too small, Wolfe conditions (or strong Wolfe conditions) add a curvature condition: <pre><code>\\big| \\nabla f(x_k + \\alpha_k p_k)^T p_k \\big| \\le c_2 \\big| \\nabla f(x_k)^T p_k \\big|,\n</code></pre> with \\(c_2\\) typically in \\((c, 1)\\).</p>"},{"location":"optimization/unconstrained/line_vs_trust/#12-algorithm-backtracking-line-search","title":"1.2. Algorithm: Backtracking Line Search","text":"<p>A common practical approach is the backtracking line search: 1. Initialize: Set \\(\\alpha = \\alpha_{\\text{init}}\\) (often 1) and choose $ \\tau \\in (0,1) $ (e.g., 0.5). 2. Iterate: While the Armijo condition is not met, update \\(\\alpha \\leftarrow \\tau \\alpha\\). 3. Return: Use the final \\(\\alpha\\) as \\(\\alpha_k\\).</p>"},{"location":"optimization/unconstrained/line_vs_trust/#13-code-examples","title":"1.3. Code Examples","text":"<p>=== julia <pre><code># Define the objective function and its gradient.\nf(x) = (x - 5)^2\ngrad_f(x) = 2*(x - 5)\n\n# Backtracking line search function.\nfunction backtracking_line_search(f, grad_f, x, p; \u03b1_init=1.0, \u03c4=0.5, c=1e-4)\n    \u03b1 = \u03b1_init\n    f_x = f(x)\n    g_x = grad_f(x)\n    # Ensure p is a descent direction.\n    if dot(g_x, p) \u2265 0\n        error(\"Search direction is not a descent direction.\")\n    end\n    while f(x + \u03b1*p) &gt; f_x + c*\u03b1*dot(g_x, p)\n        \u03b1 *= \u03c4\n    end\n    return \u03b1\nend\n\n# Example: Steepest descent for f(x) = (x-5)^2.\nx0 = 0.0\np = -grad_f(x0)  # Steepest descent direction.\n\u03b1 = backtracking_line_search(f, grad_f, x0, p)\nx1 = x0 + \u03b1 * p\n\nprintln(\"Julia: x0 = \", x0)\nprintln(\"Julia: Step size \u03b1 = \", \u03b1)\nprintln(\"Julia: New iterate x1 = \", x1)\n</code></pre></p> <p>=== python</p> <pre><code>import numpy as np\n\ndef f(x):\n    return (x - 5)**2\n\ndef grad_f(x):\n    return 2*(x - 5)\n\ndef backtracking_line_search(f, grad_f, x, p, \u03b1_init=1.0, \u03c4=0.5, c=1e-4):\n    \u03b1 = \u03b1_init\n    f_x = f(x)\n    g_x = grad_f(x)\n    # Ensure p is a descent direction.\n    if np.dot(g_x, p) &gt;= 0:\n        raise ValueError(\"Search direction is not a descent direction.\")\n    while f(x + \u03b1 * p) &gt; f_x + c * \u03b1 * np.dot(g_x, p):\n        \u03b1 *= \u03c4\n    return \u03b1\n\n# Example: Steepest descent for f(x) = (x-5)^2.\nx0 = 0.0\np = -grad_f(x0)  # Steepest descent direction.\n\u03b1 = backtracking_line_search(f, grad_f, x0, p)\nx1 = x0 + \u03b1 * p\n\nprint(\"Python: x0 =\", x0)\nprint(\"Python: Step size \u03b1 =\", \u03b1)\nprint(\"Python: New iterate x1 =\", x1)\n</code></pre>"},{"location":"optimization/unconstrained/line_vs_trust/#2-trust-region-methods","title":"2. Trust Region Methods","text":""},{"location":"optimization/unconstrained/line_vs_trust/#21-concept-and-motivation","title":"2.1. Concept and Motivation","text":"<p>Trust region methods approach the step selection problem differently. Instead of choosing a step size along a fixed direction, these methods: - Build a local model: Typically a quadratic model of $ f $ around $ x_k $:   <pre><code>m_k(p) = f(x_k) + \\nabla f(x_k)^T p + \\frac{1}{2} p^T B_k p,\n</code></pre>   where $ B_k $ is either the Hessian \\(\\nabla^2 f(x_k)\\) or an approximation thereof. - Restrict the step: Only consider steps $ p $ that lie within a neighborhood of $ x_k $ of radius \\(\\Delta_k\\):   <pre><code>\\|p\\| \\le \\Delta_k.\n</code></pre> - Solve the subproblem: Find   <pre><code>p_k = \\arg\\min_{\\|p\\| \\le \\Delta_k} m_k(p).\n</code></pre> - Update the iterate: Set   <pre><code>x_{k+1} = x_k + p_k.\n</code></pre></p> <p>The idea is that the quadratic model is assumed to be a good approximation within the \"trust region.\" If the model predicts a good decrease and the actual function decreases sufficiently, the trust region can be expanded; otherwise, it is contracted.</p>"},{"location":"optimization/unconstrained/line_vs_trust/#22-key-components","title":"2.2. Key Components","text":"<ul> <li> <p>Trust Region Subproblem:   The quadratic subproblem is   <pre><code>\\min_{p} \\; m_k(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta_k.\n</code></pre>   This subproblem is usually solved approximately (e.g., via the dogleg method, conjugate gradient, or truncated CG).</p> </li> <li> <p>Trust Region Radius \\(\\Delta_k\\):   After computing $ p_k $, we compare the actual reduction in $ f $ to the predicted reduction by $ m_k(p) $. Based on the ratio   <pre><code>\\rho_k = \\frac{f(x_k) - f(x_k+p_k)}{m_k(0) - m_k(p_k)},\n</code></pre>   we adjust \\(\\Delta_k\\):</p> </li> <li>If \\(\\rho_k\\) is high (model was good), increase \\(\\Delta_k\\).</li> <li>If \\(\\rho_k\\) is low (model was poor), decrease \\(\\Delta_k\\).</li> </ul>"},{"location":"optimization/unconstrained/line_vs_trust/#23-code-sketch","title":"2.3. Code Sketch","text":"<p>Below is a simplified pseudocode outline and code snippet in Julia for a trust region step.</p> <p>Pseudocode: 1. At iterate \\(x_k\\), build a quadratic model:    <pre><code>m_k(p) = f(x_k) + \\nabla f(x_k)^T p + \\frac{1}{2} p^T B_k p.\n</code></pre> 2. Solve the subproblem:    <pre><code>p_k = \\arg\\min_{\\|p\\| \\le \\Delta_k} m_k(p).\n</code></pre> 3. Compute the ratio:    <pre><code>\\rho_k = \\frac{f(x_k) - f(x_k+p_k)}{m_k(0) - m_k(p_k)}.\n</code></pre> 4. Update \\(x_{k+1} = x_k + p_k\\) and adjust \\(\\Delta_k\\) based on \\(\\rho_k\\).</p>"},{"location":"optimization/unconstrained/line_vs_trust/#example-pseudocode-style","title":"Example (Pseudocode Style)","text":"<p>=== julia <pre><code>function trust_region_step(f, grad_f, B, x, \u0394)\n    # Define the quadratic model:\n    m(p) = f(x) + dot(grad_f(x), p) + 0.5 * dot(p, B * p)\n\n    # (For illustration, assume we solve the subproblem exactly; in practice, use a solver.)\n    # Here we use the dogleg method or a truncated CG method.\n    p = solve_trust_region_subproblem(grad_f(x), B, \u0394)\n\n    # Compute actual and predicted reduction:\n    actual_reduction = f(x) - f(x + p)\n    predicted_reduction = m(zeros(length(x))) - m(p)\n    \u03c1 = actual_reduction / predicted_reduction\n\n    # Update trust region radius (simplified rule):\n    if \u03c1 &lt; 0.25\n        \u0394_new = 0.5 * \u0394\n    elseif \u03c1 &gt; 0.75 &amp;&amp; norm(p) == \u0394\n        \u0394_new = min(2 * \u0394, \u0394_max)  # assume some \u0394_max is defined\n    else\n        \u0394_new = \u0394\n    end\n    return x + p, \u0394_new, \u03c1\nend\n</code></pre></p> <p>Note: In an actual implementation, the subproblem solver (e.g. dogleg or truncated CG) would be used to compute \\(p\\). Packages like JSOSolvers.jl and Optimization.jl provide robust trust region methods.</p> <p>=== python <pre><code>import numpy as np\n\ndef quadratic_model(f, grad, B, x, p):\n    return f(x) + np.dot(grad(x), p) + 0.5 * np.dot(p, B @ p)\n\ndef trust_region_step(f, grad, B, x, \u0394, \u0394_max=1.0):\n    # For illustration, assume we solve the trust region subproblem approximately.\n    # Here we use the Cauchy point as a simple solution:\n    g = grad(x)\n    p_cauchy = - (\u0394 / np.linalg.norm(g)) * g\n    p = p_cauchy  # In practice, use dogleg or truncated CG.\n\n    actual_reduction = f(x) - f(x + p)\n    predicted_reduction = quadratic_model(f, grad, B, x, np.zeros_like(x)) - quadratic_model(f, grad, B, x, p)\n    \u03c1 = actual_reduction / predicted_reduction\n\n    if \u03c1 &lt; 0.25:\n        \u0394_new = 0.5 * \u0394\n    elif \u03c1 &gt; 0.75 and np.linalg.norm(p) &gt;= \u0394:\n        \u0394_new = min(2 * \u0394, \u0394_max)\n    else:\n        \u0394_new = \u0394\n    return x + p, \u0394_new, \u03c1\n\n# Example: minimize f(x) = (x-5)^2, with constant Hessian B = 2.\nf = lambda x: (x - 5)**2\ngrad = lambda x: 2*(x - 5)\nB = np.array([[2.0]])\n\nx0 = np.array([0.0])\n\u0394 = 1.0\nx1, \u0394_new, \u03c1 = trust_region_step(f, grad, B, x0, \u0394)\nprint(\"Python: New iterate x1 =\", x1)\nprint(\"Python: New trust region radius \u0394 =\", \u0394_new)\nprint(\"Python: Ratio \u03c1 =\", \u03c1)\n</code></pre></p>"},{"location":"optimization/unconstrained/line_vs_trust/#3-comparison-line-search-vs-trust-region","title":"3. Comparison: Line Search vs. Trust Region","text":"<ul> <li>Line Search:</li> <li>Idea: Move along a fixed search direction $ p_k $ by choosing a step size \\(\\alpha_k\\).</li> <li>Key Conditions: Armijo and Wolfe (or strong Wolfe) conditions.</li> <li>Pros: Often simple to implement; works well when the local model is valid along the entire line.</li> <li> <p>Cons: Requires careful selection of \\(\\alpha_{\\text{init}}\\) and may involve multiple function evaluations per iteration.</p> </li> <li> <p>Trust Region:</p> </li> <li>Idea: Build a local (often quadratic) model $ m_k(p) $ and trust that model only within a ball (trust region) of radius \\(\\Delta_k\\). The step $ p_k $ is chosen by approximately minimizing $ m_k(p) $ subject to \\(\\|p\\|\\leq \\Delta_k\\).</li> <li>Pros: Naturally limits the step size when the model is unreliable; robust in the presence of nonconvexity.</li> <li>Cons: The subproblem may be harder to solve; requires mechanisms to adjust the radius \\(\\Delta_k\\).</li> </ul> <p>Both strategies are widely used. The choice depends on the problem structure and practical considerations. In many modern solvers, hybrid strategies exist, and libraries like JSOSolvers.jl offer both line search and trust region solvers for unconstrained optimization.</p>"},{"location":"optimization/unconstrained/line_vs_trust/#4-further-reading-and-references","title":"4. Further Reading and References","text":"<p>For a deeper dive into these strategies and their analysis, consult:</p> <ul> <li>Nocedal, Jorge, and Stephen J. Wright. Numerical Optimization. Springer, 2006.</li> <li>Armijo, Larry. \"Minimization of Functions Having Lipschitz Continuous First Partial Derivatives.\" Pacific Journal of Mathematics, 1966.</li> <li>Wolfe Conditions \u2013 Wikipedia: https://en.wikipedia.org/wiki/Wolfe_conditions</li> <li>Backtracking Line Search \u2013 Wikipedia: https://en.wikipedia.org/wiki/Backtracking_line_search</li> <li>JSOSolvers.jl and Optimization.jl: JSOSolvers.jl Optimization.jl Documentation</li> </ul> <p>Additionally, explore related lecture notes on algorithms such as MINRES (for linear systems) and dedicated notes on line search strategies.</p>"},{"location":"optimization/unconstrained/line_vs_trust/#conclusion","title":"Conclusion","text":"<p>In these lecture notes, we have discussed two primary strategies for step size selection in unconstrained optimization:</p> <ul> <li>Line Search Methods rely on finding an appropriate \\(\\alpha\\) along a fixed search direction using conditions such as Armijo and Wolfe conditions.  </li> <li>Trust Region Methods build a local model of the objective and restrict the step to lie within a region where that model is trustworthy.</li> </ul> <p>Both strategies have their theoretical merits and practical trade-offs. Modern optimization solvers often integrate these ideas (or even hybrid methods) to achieve robust and efficient performance.</p> <p>These notes include examples in both Julia and Python to help illustrate the concepts. For further exploration, links to related topics and additional references are provided.</p>"},{"location":"optimization/unconstrained/minres/","title":"Comprehensive Lecture on MINRES and Newton-MR","text":"<p>It explores advanced Krylov subspace methods for symmetric systems\u2014specifically MINRES, CG, and CR\u2014and shows how these methods are incorporated into a Newton-type method (Newton-MR) for nonconvex smooth unconstrained optimization. Special emphasis is placed on detecting negative curvature via MINRES and the strategy to switch to a line search when such curvature is encountered.</p>"},{"location":"optimization/unconstrained/minres/#1-overview-of-krylov-subspace-methods-for-symmetric-systems","title":"1. Overview of Krylov Subspace Methods for Symmetric Systems","text":"<p>When solving a linear system</p> <pre><code>Ax = b,\n</code></pre> <p>iterative methods based on Krylov subspaces are preferred for large-scale or sparse problems. In symmetric settings, three key methods are:</p> <ul> <li> <p>Conjugate Gradient (CG):   Designed for symmetric positive definite (SPD) systems. CG minimizes the energy (or \\(A\\)-norm) error in the Krylov subspace. Its convergence rate is \\(O(\\sqrt{\\kappa(A)})\\) in the best case, where \\(\\kappa(A)\\) is the condition number.</p> </li> <li> <p>Conjugate Residual (CR):   Applicable to symmetric systems\u2014including indefinite ones\u2014CR minimizes the residual norm \\(\\|b-Ax\\|\\) over the Krylov subspace. While it is closely related to MINRES, its recurrences require slightly more storage.</p> </li> <li> <p>MINRES (Minimum Residual Method):   Tailored for symmetric systems (even if indefinite), MINRES directly minimizes the 2-norm of the residual   <pre><code>r(x) = b - Ax,\n</code></pre>   by projecting the problem onto a Krylov subspace generated via the Lanczos process\u2014a specialized Arnoldi iteration for symmetric matrices. MINRES is especially attractive because it efficiently handles negative or zero curvature situations that arise in nonconvex optimization.</p> </li> </ul> <p>Key Remark: While CG is optimal for SPD systems, MINRES extends to indefinite matrices and inherently detects directions of non-positive curvature.</p> <p>For additional background on MINRES, see Paige and Saunders (1975) and the Wikipedia article on Minimal Residual Method.</p>"},{"location":"optimization/unconstrained/minres/#2-in-depth-the-minres-method","title":"2. In-Depth: The MINRES Method","text":""},{"location":"optimization/unconstrained/minres/#21-derivation-via-the-lanczos-process","title":"2.1. Derivation via the Lanczos Process","text":"<p>For a symmetric matrix \\(A\\), the Arnoldi process simplifies to the Lanczos iteration. Starting with the normalized residual</p> <pre><code>q_1 = \\frac{r_0}{\\|r_0\\|},\n</code></pre> <p>the Lanczos process generates an orthonormal basis</p> <pre><code>Q_m = [q_1, q_2, \\dots, q_m]\n</code></pre> <p>for the Krylov subspace</p> <pre><code>\\mathcal{K}_m(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, A^2r_0, \\dots, A^{m-1}r_0\\}.\n</code></pre> <p>In exact arithmetic, one obtains the relation</p> <pre><code>A Q_m = Q_{m+1} \\tilde{T}_m,\n</code></pre> <p>where \\(\\tilde{T}_m\\) is an \\((m+1) \\times m\\) tridiagonal matrix. Any approximate solution in the affine subspace \\(x_0 + \\mathcal{K}_m\\) can then be written as</p> <pre><code>x_m = x_0 + Q_m y,\n</code></pre> <p>with \\(y \\in \\mathbb{R}^m\\). The residual becomes</p> <pre><code>r_m = b - Ax_m = r_0 - A Q_m y = \\|r_0\\| \\left(e_1 - \\tilde{T}_m y\\right),\n</code></pre> <p>so that minimizing \\(\\|r_m\\|\\) is equivalent to solving a small least-squares problem:</p> <pre><code>y_m = \\operatorname*{argmin}_{y\\in\\mathbb{R}^m} \\left\\|\\|r_0\\|e_1 - \\tilde{T}_m y\\right\\|_2.\n</code></pre>"},{"location":"optimization/unconstrained/minres/#22-convergence-properties","title":"2.2. Convergence Properties","text":"<p>For symmetric (even indefinite) matrices, convergence bounds for MINRES can be derived via polynomial approximation. For example, if the eigenvalues of \\(A\\) are divided into positive and negative parts, one may have a bound of the form</p> <pre><code>\\frac{\\|r_m\\|}{\\|r_0\\|} \\le \\left(\\frac{\\sqrt{\\kappa_+ \\kappa_-} - 1}{\\sqrt{\\kappa_+ \\kappa_-} + 1}\\right)^{\\lfloor m/2 \\rfloor},\n</code></pre> <p>where - \\(\\kappa_+ = \\frac{\\max_{\\lambda \\in \\Lambda_+}|\\lambda|}{\\min_{\\lambda \\in \\Lambda_+}|\\lambda|}\\) (for positive eigenvalues), and - \\(\\kappa_- = \\frac{\\max_{\\lambda \\in \\Lambda_-}|\\lambda|}{\\min_{\\lambda \\in \\Lambda_-}|\\lambda|}\\) (for negative eigenvalues).</p> <p>For SPD matrices, one recovers a similar bound to that for CG:</p> <pre><code>\\frac{\\|r_m\\|}{\\|r_0\\|} \\le 2 \\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^m.\n</code></pre> <p>These bounds illustrate that the convergence rate depends on the spectral properties of \\(A\\); clustering of eigenvalues near zero can slow convergence.</p>"},{"location":"optimization/unconstrained/minres/#3-comparison-minres-vs-cg-vs-cr","title":"3. Comparison: MINRES vs. CG vs. CR","text":"Method Matrix Requirements Minimization Target Recurrence Complexity Key Strengths CG Symmetric positive definite Energy norm error \\(\\|x - x^*\\|_A\\) Short recurrence (\\(O(1)\\) per iteration) Highly efficient for SPD systems; optimal for quadratic minimization. CR Symmetric (SPD or indefinite) 2-norm residual \\(\\|b - Ax\\|\\) Similar to CG with slightly more storage Minimizes the residual; closely related to MINRES. MINRES Symmetric (including indefinite) Direct minimization of \\(\\|b - Ax\\|_2\\) Short recurrence via Lanczos (comparable to CG) Robust for indefinite systems; inherently detects negative curvature. <p>In many optimization applications\u2014especially within a Newton-type method\u2014the Hessian \\(H_k\\) may be indefinite. In such cases, CG may not be applicable or requires modifications, whereas MINRES naturally handles these situations.</p>"},{"location":"optimization/unconstrained/minres/#4-newton-mr-a-newton-type-method-for-nonconvex-optimization","title":"4. Newton-MR: A Newton-Type Method for Nonconvex Optimization","text":"<p>Newton-MR is a variant of Newton\u2019s method that employs MINRES to solve the Newton system in nonconvex optimization problems. This is particularly useful when the Hessian</p> <pre><code>H_k = \\nabla^2 f(x_k)\n</code></pre> <p>is indefinite.</p>"},{"location":"optimization/unconstrained/minres/#41-motivation","title":"4.1. Motivation","text":"<ul> <li> <p>Indefinite Hessians in Nonconvex Problems:   In nonconvex settings, the Hessian can have negative or zero eigenvalues. Traditional Newton methods (or Newton-CG) may fail or require modifications such as adding a damping term, which may slow down convergence.</p> </li> <li> <p>Leveraging MINRES:   MINRES is designed for symmetric indefinite systems. It naturally minimizes the residual norm and, via its Lanczos process, can detect directions of negative curvature. This detection is key to ensuring that the algorithm does not take a full Newton step that could lead to ascent or non-descent.</p> </li> </ul>"},{"location":"optimization/unconstrained/minres/#42-detecting-negative-or-zero-curvature-in-minres","title":"4.2. Detecting Negative or Zero Curvature in MINRES","text":"<p>During the MINRES iterations, the Lanczos process computes recurrence coefficients \\(\\alpha_j\\) (diagonal entries) and \\(\\beta_j\\) (off-diagonal entries) that form the tridiagonal matrix \\(\\tilde{T}_m\\). These coefficients carry curvature information:</p> <ul> <li> <p>Rayleigh Quotient Approximation:   The diagonal coefficients \\(\\alpha_j\\) approximate the Rayleigh quotient in the Krylov subspace. If at any iteration \\(j\\) an \\(\\alpha_j\\) becomes non-positive (or nearly zero), it signals that the Hessian has a direction of non-positive (negative or zero) curvature.</p> </li> <li> <p>Practical Detection:   In implementations, MINRES monitors these recurrence coefficients. Once a coefficient satisfies   <pre><code>\\alpha_j \\leq \\delta \\quad (\\delta \\approx 0),\n</code></pre>   this serves as a flag that a negative curvature direction has been encountered.</p> </li> </ul>"},{"location":"optimization/unconstrained/minres/#43-switching-to-a-line-search","title":"4.3. Switching to a Line Search","text":"<p>When negative or zero curvature is detected, Newton-MR adopts a safeguard by switching to a line search along the direction associated with the detected curvature:</p> <ol> <li> <p>Extract Negative Curvature Direction:    When the MINRES iteration indicates non-positive curvature, the algorithm identifies a direction \\(d\\) (often related to the most recent Krylov basis vector) that exhibits negative curvature.</p> </li> <li> <p>Line Search Procedure:    Instead of using the full Newton step \\(p\\) computed via MINRES, the algorithm performs a line search along \\(d\\) to ensure that the update    <pre><code>x_{k+1} = x_k + \\alpha d\n</code></pre>    yields a sufficient decrease in the objective function \\(f\\).</p> </li> <li> <p>Ensuring Sufficient Decrease:    A common strategy is to use an Armijo condition:    <pre><code>f(x_k + \\alpha d) \\le f(x_k) + c \\, \\alpha \\, g_k^T d,\n</code></pre>    with \\(c \\in (0,1)\\). This safeguards the descent property even when the Hessian is indefinite.</p> </li> <li> <p>Adaptive Strategy:    The paper details an adaptive strategy whereby the MINRES subproblem is solved only until a curvature condition is violated. At that point, the algorithm \u201cswitches off\u201d the standard Newton update and reverts to a safeguarded line search, ensuring overall descent and robust convergence.</p> </li> </ol>"},{"location":"optimization/unconstrained/minres/#5-detailed-pseudocode-for-newton-mr-with-curvature-detection","title":"5. Detailed Pseudocode for Newton-MR with Curvature Detection","text":"<pre><code>Algorithm Newton-MR:\nInput: Initial point \\(x_0\\), tolerance \\(\\epsilon\\), maximum iterations \\(K\\).\n\nfor \\(k = 0, 1, \\dots, K-1\\):\n  1. Compute gradient: \\(g_k = \\nabla f(x_k)\\).\n  2. If \\(\\|g_k\\| \\le \\epsilon\\), terminate.\n  3. Compute Hessian: \\(H_k = \\nabla^2 f(x_k)\\).\n  4. Solve \\(H_k p = -g_k\\) approximately using MINRES:\n     a. Start with \\(p^{(0)} = 0\\).\n     b. Run MINRES, and at each iteration monitor the Lanczos coefficient \\(\\alpha_j\\).\n     c. If any \\(\\alpha_j \\le \\delta\\) (indicating negative/zero curvature), extract the corresponding direction \\(d\\).\n  5. **If negative curvature detected:**\n     - Switch to a safeguarded line search along \\(d\\) (instead of the full Newton step).\n  6. **Else:**\n     - Accept the computed \\(p\\) as the Newton direction.\n  7. Perform a line search to determine step size \\(\\alpha_k\\) that satisfies a sufficient decrease condition.\n  8. Update \\(x_{k+1} = x_k + \\alpha_k \\times\\) (chosen direction).\nend for\n</code></pre>"},{"location":"optimization/unconstrained/minres/#6-illustrative-julia-implementation","title":"6. Illustrative Julia Implementation","text":"<p>Below is a Julia code snippet that outlines the Newton-MR algorithm. This version includes comments on detecting negative curvature and switching to a line search. (In practice, more advanced curvature-detection logic may be integrated.)</p> <pre><code>using LinearAlgebra, IterativeSolvers\n\n\"\"\"\n    newtonMR(f, grad, hess, x0; tol, maxit)\n\nNewton-MR solver for unconstrained optimization.\n- `f`: objective function\n- `grad`: gradient function\n- `hess`: Hessian function\n- `x0`: initial point\n- `tol`: tolerance on the gradient norm\n- `maxit`: maximum number of outer iterations\n\"\"\"\nfunction newtonMR(f, grad, hess, x0; tol=1e-6, maxit=50)\n    x = x0\n    for k in 1:maxit\n        g = grad(x)\n        println(\"Iteration $k: ||grad|| = \", norm(g))\n        if norm(g) &lt; tol\n            println(\"Convergence achieved at iteration $k\")\n            return x\n        end\n\n        H = hess(x)\n        # Define Hessian operator for MINRES\n        H_operator(v) = H * v\n\n        # Solve H * p = -g using MINRES; monitor MINRES coefficients.\n        # (In a full implementation, one would check the recurrence coefficients here.)\n        p, flag = minres(H_operator, -g, tol=1e-4, maxiter=length(x))\n        if flag != 0\n            println(\"MINRES did not converge at iteration $k; flag = $flag\")\n            break\n        end\n\n        # Here we assume that our MINRES routine internally monitors the Lanczos recurrence.\n        # If negative curvature is detected (e.g., a coefficient falls below a threshold \u03b4),\n        # the algorithm would set a flag and extract the corresponding direction d.\n        # For this illustrative code, we simply check a condition (dummy check):\n        negative_curvature_detected = false  # Replace with actual detection logic\n        if negative_curvature_detected\n            println(\"Negative curvature detected at iteration $k. Switching to line search.\")\n            # Assume d is the extracted negative curvature direction (here, simply use p)\n            d = p  # In practice, d would be chosen based on the curvature information\n        else\n            d = p\n        end\n\n        # Backtracking line search using Armijo condition:\n        \u03b1 = 1.0\n        while f(x + \u03b1 * d) &gt; f(x) - 1e-4 * \u03b1 * dot(g, d)\n            \u03b1 *= 0.5\n        end\n\n        x += \u03b1 * d\n    end\n    return x\nend\n\n# Example: quadratic function (SPD case; for illustration)\nf(x) = 0.5 * dot(x, x)         # f(x) = 0.5 x\u1d40x\ngrad(x) = x                    # grad f(x) = x\nhess(x) = I                    # Hessian is the identity\n\nx0 = randn(10)\nsolution = newtonMR(f, grad, hess, x0)\nprintln(\"Solution: \", solution)\n</code></pre> <p>Note: In real nonconvex applications, the Hessian may be indefinite. The MINRES routine would then monitor the Lanczos coefficients (\\(\\alpha_j\\)) to detect negative curvature (when any \\(\\alpha_j\\) falls below a small threshold \\(\\delta\\)). Upon such detection, the algorithm switches from the full Newton step to a safeguarded line search along the direction associated with the negative curvature, ensuring a sufficient decrease in \\(f\\).</p>"},{"location":"optimization/unconstrained/minres/#7-concluding-remarks","title":"7. Concluding Remarks","text":"<ul> <li> <p>MINRES as a Robust Subproblem Solver:   MINRES directly minimizes the residual norm and is well suited for symmetric indefinite matrices. Its derivation via the Lanczos process yields efficient short recurrences and, crucially, the ability to detect directions of non-positive curvature.</p> </li> <li> <p>Comparative Insights:   While CG is optimal for SPD systems, MINRES (and CR) extend applicability to broader classes of symmetric matrices. Their convergence properties\u2014governed by the distribution of eigenvalues\u2014illustrate how negative curvature can be detected and exploited.</p> </li> <li> <p>Newton-MR Advantages:   Newton-MR leverages MINRES to solve the Newton system even when the Hessian is indefinite. When MINRES detects negative or zero curvature (via non-positive Lanczos coefficients), the algorithm switches to a line search along a negative curvature direction, ensuring descent and robust convergence. This adaptive mechanism underpins the complexity guarantees provided by the method.</p> </li> </ul>"},{"location":"optimization/unconstrained/minres/#references","title":"References","text":"<ol> <li> <p>Liu, Yang, and Fred Roosta. A Newton-MR Algorithm with Complexity Guarantees for Nonconvex Smooth Unconstrained Optimization.    ArXiv preprint, arXiv:2208.07095. \ue200cite\ue202turn0search1\ue201</p> </li> <li> <p>Paige, C. C., and M. A. Saunders. Solution of Sparse Indefinite Systems of Linear Equations.    SIAM Journal on Numerical Analysis, 12 (1975): 617\u2013629.</p> </li> <li> <p>Saad, Y. Iterative Methods for Sparse Linear Systems.    SIAM, 2003.</p> </li> <li> <p>Wikipedia. Minimal Residual Method. https://en.wikipedia.org/wiki/Minimal_residual_method (Accessed: November 2023). \ue200cite\ue202turn0search0\ue201</p> </li> <li> <p>Fong, D. C.-L., and M. A. Saunders. MINRES: A Krylov Subspace Method for Symmetric Linear Systems.    (For further background on MINRES.)</p> </li> </ol>"}]}